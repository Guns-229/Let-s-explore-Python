{"GCP Professional Cloud Architect": [{"id": 1, "question": "Which of the following services provides real-time messaging?", "choices": [{"id": 1511, "choice": " Cloud Pub/Sub\n", "answer": 1}, {"id": 1512, "choice": "Big Query\n", "answer": 0}, {"id": 1513, "choice": "App Engine\n", "answer": 0}, {"id": 1514, "choice": "Datastore", "answer": 0}]}, {"id": 2, "question": "Which of the following tasks would Nearline Storage be well suited for?", "choices": [{"id": 1515, "choice": " A mounted Linux file system\n", "answer": 0}, {"id": 1516, "choice": "Image assets for a high traffic website\n", "answer": 0}, {"id": 1517, "choice": "Frequently read files\n", "answer": 0}, {"id": 1518, "choice": "Infrequently read data backups", "answer": 1}]}, {"id": 3, "question": "Which of the following products will allow you to administer your projects through a browser based command-\nline?", "choices": [{"id": 1519, "choice": " Cloud Datastore\n", "answer": 0}, {"id": 1520, "choice": "Cloud Command-line\n", "answer": 0}, {"id": 1521, "choice": "Cloud Terminal\n", "answer": 0}, {"id": 1522, "choice": "Cloud Shell", "answer": 1}]}, {"id": 4, "question": "Cloud SQL is based on which database engine?", "choices": [{"id": 1523, "choice": " Microsoft SQL Server\n", "answer": 0}, {"id": 1524, "choice": "MySQL\n", "answer": 1}, {"id": 1525, "choice": "Oracle\n", "answer": 0}, {"id": 1526, "choice": "Informix", "answer": 0}]}, {"id": 5, "question": "Which of the following products will allow you to perform live debugging without stopping your application?", "choices": [{"id": 1527, "choice": " App Engine Active Debugger (AEAD)\n", "answer": 0}, {"id": 1528, "choice": "Stackdriver Debugger\n", "answer": 1}, {"id": 1529, "choice": "Code Inspector\n", "answer": 0}, {"id": 1530, "choice": "Pause IT", "answer": 0}]}, {"id": 6, "question": "Which of these options is not a valid Cloud Storage class?", "choices": [{"id": 1531, "choice": " Glacier Storage\n", "answer": 1}, {"id": 1532, "choice": "Nearline Storage\n", "answer": 0}, {"id": 1533, "choice": "Coldline Storage\n", "answer": 0}, {"id": 1534, "choice": "Regional Storage", "answer": 0}]}, {"id": 7, "question": "Regarding Cloud Storage, which option allows any user to access to a Cloud Storage resource for a limited\ntime, using a specific URL?", "choices": [{"id": 1535, "choice": " Open Buckets\n", "answer": 0}, {"id": 1536, "choice": "Temporary Resources\n", "answer": 0}, {"id": 1537, "choice": "Signed URLs\n", "answer": 1}, {"id": 1538, "choice": "Temporary URLs", "answer": 0}]}, {"id": 8, "question": "Of the options given, which is a NoSQL database?", "choices": [{"id": 1539, "choice": " Cloud Datastore\n", "answer": 1}, {"id": 1540, "choice": "Cloud SQL\n", "answer": 0}, {"id": 1541, "choice": "All of the given options\n", "answer": 0}, {"id": 1542, "choice": "Cloud Storage", "answer": 0}]}, {"id": 9, "question": "Container Engine allows orchastration of what type of containers?", "choices": [{"id": 1543, "choice": " Blue Whale\n", "answer": 0}, {"id": 1544, "choice": "LXC\n", "answer": 0}, {"id": 1545, "choice": "BSD Jails\n", "answer": 0}, {"id": 1546, "choice": "Docker", "answer": 1}]}, {"id": 10, "question": "Regarding Cloud IAM, what type of role(s) are available?", "choices": [{"id": 1547, "choice": " Basic roles and Compiled roles\n", "answer": 0}, {"id": 1548, "choice": "Primitive roles and Predefined roles\n", "answer": 1}, {"id": 1549, "choice": "Simple roles\n", "answer": 0}, {"id": 1550, "choice": "Basic roles and Curated roles", "answer": 0}]}, {"id": 11, "question": "Which of the follow products will allow you to host a static website?", "choices": [{"id": 1551, "choice": " Cloud SDK\n", "answer": 0}, {"id": 1552, "choice": "Cloud Endpoints\n", "answer": 0}, {"id": 1553, "choice": "Cloud Storage\n", "answer": 1}, {"id": 1554, "choice": "Cloud Datastore", "answer": 0}]}, {"id": 12, "question": "Container Engine is built on which open source system?", "choices": [{"id": 1555, "choice": " Swarm\n", "answer": 0}, {"id": 1556, "choice": "Kubernetes\n", "answer": 1}, {"id": 1557, "choice": "Docker Orchastrate\n", "answer": 0}, {"id": 1558, "choice": "Mesos", "answer": 0}]}, {"id": 13, "question": "Cloud Source Repositories provide a hosted version of which version control system?", "choices": [{"id": 1559, "choice": " Git\n", "answer": 1}, {"id": 1560, "choice": "RCS\n", "answer": 0}, {"id": 1561, "choice": "SVN\n", "answer": 0}, {"id": 1562, "choice": "Mercurial", "answer": 0}]}, {"id": 14, "question": "Which of the following is an analytics data warehouse?", "choices": [{"id": 1563, "choice": " Cloud SQL\n", "answer": 0}, {"id": 1564, "choice": "Big Query\n", "answer": 1}, {"id": 1565, "choice": "Datastore\n", "answer": 0}, {"id": 1566, "choice": "Cloud Storage", "answer": 0}]}, {"id": 15, "question": "Which service offers the ability to create and run virtual machines?", "choices": [{"id": 1567, "choice": " Google Virtualization Engine\n", "answer": 0}, {"id": 1568, "choice": "Compute Containers\n", "answer": 0}, {"id": 1569, "choice": "VM Engine\n", "answer": 0}, {"id": 1570, "choice": "Compute Engine", "answer": 1}]}, {"id": 16, "question": "Which of the following is not helpful for mitigating the impact of an unexpected failure or reboot?", "choices": [{"id": 1571, "choice": " Use persistent disks\n", "answer": 0}, {"id": 1572, "choice": "Configure tags and labels\n", "answer": 1}, {"id": 1573, "choice": "Use startup scripts to re-configure the system as needed\n", "answer": 0}, {"id": 1574, "choice": "Back up your data", "answer": 0}]}, {"id": 17, "question": "Single sign-on (SSO) with G Suite is based on _____?", "choices": [{"id": 1575, "choice": " SAML2\n", "answer": 1}, {"id": 1576, "choice": "JWT\n", "answer": 0}, {"id": 1577, "choice": "Service accounts\n", "answer": 0}, {"id": 1578, "choice": "JSON", "answer": 0}]}, {"id": 18, "question": "Which tool allows you to sync data in your Google domain with Active Directory?", "choices": [{"id": 1579, "choice": " Google Cloud Directory Sync (GCDS)\n", "answer": 1}, {"id": 1580, "choice": "Google Active Directory (GAD)\n", "answer": 0}, {"id": 1581, "choice": "Google Domain Sync Service\n", "answer": 0}, {"id": 1582, "choice": "Google LDAP Sync", "answer": 0}]}, {"id": 19, "question": "Regarding Cloud Storage: which of the following allows for time-limited access to buckets and objects without a\nGoogle account?", "choices": [{"id": 1583, "choice": " Signed URLs\n", "answer": 1}, {"id": 1584, "choice": "gsutil\n", "answer": 0}, {"id": 1585, "choice": "Single sign-on\n", "answer": 0}, {"id": 1586, "choice": "Temporary Storage Accounts", "answer": 0}]}, {"id": 20, "question": "Which of the following is a virtual machine instance that can be terminated by Compute Engine without\nwarning?", "choices": [{"id": 1587, "choice": " A preemptible VM\n", "answer": 1}, {"id": 1588, "choice": "A shared-core VM\n", "answer": 0}, {"id": 1589, "choice": "A high-cpu VM\n", "answer": 0}, {"id": 1590, "choice": "A standard VM", "answer": 0}]}, {"id": 21, "question": "Regarding Compute Engine: What is a managed instance group?", "choices": [{"id": 1591, "choice": " A managed instance group combines existing instances of different configurations into one manageable\ngroup\n", "answer": 0}, {"id": 1592, "choice": "A managed instance group uses an instance template to create identical instances\n", "answer": 1}, {"id": 1593, "choice": "A managed instance group creates a firewall around instances\n", "answer": 0}, {"id": 1594, "choice": "A managed instance group is a set of servers used exclusively for batch processing", "answer": 0}]}, {"id": 22, "question": "What type of firewall rule(s) does Google Cloud's networking support?", "choices": [{"id": 1595, "choice": " deny\n", "answer": 0}, {"id": 1596, "choice": "allow, deny & filtered\n", "answer": 0}, {"id": 1597, "choice": "allow\n", "answer": 0}, {"id": 1598, "choice": "allow & deny", "answer": 1}]}, {"id": 23, "question": "How are subnetworks different than the legacy networks?", "choices": [{"id": 1599, "choice": " They're the same, only the branding is different\n", "answer": 0}, {"id": 1600, "choice": "Each subnetwork controls the IP address range used for instances that are allocated to that subnetwork\n", "answer": 1}, {"id": 1601, "choice": "With subnetworks IP address allocation occurs at the global network level\n", "answer": 0}, {"id": 1602, "choice": "Legacy networks are the preferred way to create networks", "answer": 0}]}, {"id": 24, "question": "Which of the following is not a valid metric for triggering autoscaling?", "choices": [{"id": 1603, "choice": " Google Cloud Pub/Sub queuing\n", "answer": 0}, {"id": 1604, "choice": "Average CPU utilization\n", "answer": 0}, {"id": 1605, "choice": "Stackdriver Monitoring metrics\n", "answer": 0}, {"id": 1606, "choice": "App Engine Task Queues", "answer": 1}]}, {"id": 25, "question": "Which of the following features makes applying firewall settings easier?", "choices": [{"id": 1607, "choice": " Service accounts\n", "answer": 0}, {"id": 1608, "choice": "Tags\n", "answer": 1}, {"id": 1609, "choice": "Metadata\n", "answer": 0}, {"id": 1610, "choice": "Labels", "answer": 0}]}, {"id": 26, "question": "What option does Cloud SQL offer to help with high availability?", "choices": [{"id": 1611, "choice": " Point-in-time recovery\n", "answer": 0}, {"id": 1612, "choice": "The AlwaysOn setting\n", "answer": 0}, {"id": 1613, "choice": "Snapshots\n", "answer": 0}, {"id": 1614, "choice": "Failover replicas", "answer": 1}]}, {"id": 27, "question": "Regarding Compute Engine: when executing a startup script on a Linux server which user does the instance\nexecute the script as?", "choices": [{"id": 1615, "choice": " ubuntu\n", "answer": 0}, {"id": 1616, "choice": "The Google provided \"gceinstance\" user\n", "answer": 0}, {"id": 1617, "choice": "Whatever user you specify in the console\n", "answer": 0}, {"id": 1618, "choice": "root", "answer": 1}]}, {"id": 28, "question": "Which of the follow methods will not cause a shutdown script to be executed?", "choices": [{"id": 1619, "choice": " When an instance shuts down through a request to the guest operating system\n", "answer": 0}, {"id": 1620, "choice": "A preemptible instance being terminated\n", "answer": 0}, {"id": 1621, "choice": "An instances.reset API call\n", "answer": 1}, {"id": 1622, "choice": "Shutting down via the cloud console", "answer": 0}]}, {"id": 29, "question": "Which type of account would you use in code when you want to interact with Google Cloud services?", "choices": [{"id": 1623, "choice": " Google group\n", "answer": 0}, {"id": 1624, "choice": "Service account\n", "answer": 1}, {"id": 1625, "choice": "Code account\n", "answer": 0}, {"id": 1626, "choice": "Google account", "answer": 0}]}, {"id": 30, "question": "Which of the following is not an IAM best practice?", "choices": [{"id": 1627, "choice": " Use primitive roles by default\n", "answer": 1}, {"id": 1628, "choice": "Treat each component of your application as a separate trust boundary\n", "answer": 0}, {"id": 1629, "choice": "Grant roles at the smallest scope needed\n", "answer": 0}, {"id": 1630, "choice": "Restrict who has access to create and manage service accounts in your project", "answer": 0}]}, {"id": 31, "question": "Which of the following would not reduce your recovery time in the event of a disaster?", "choices": [{"id": 1631, "choice": " Make it as easy as possible to adjust the DNS record to cut over to your warm standby server.\n", "answer": 0}, {"id": 1632, "choice": "Replace your warm standby server with a hot standby server.\n", "answer": 0}, {"id": 1633, "choice": "Use a highly preconfigured machine image for deploying new instances.\n", "answer": 0}, {"id": 1634, "choice": "Replace your active/active hybrid production environment (on-premises and GCP) with a warm standby\nserver.", "answer": 1}]}, {"id": 32, "question": "Which of the following is not a best practice for mitigating Denial of Service attacks on your Google Cloud\ninfrastructure?", "choices": [{"id": 1635, "choice": " Block SYN floods using Cloud Router\n", "answer": 1}, {"id": 1636, "choice": "Isolate your internal traffic from the external world\n", "answer": 0}, {"id": 1637, "choice": "Scale to absorb the attack\n", "answer": 0}, {"id": 1638, "choice": "Reduce the attack surface for your GCE deployment", "answer": 0}]}, {"id": 33, "question": "Which is the fastest instance storage option that will still be available when an instance is stopped?", "choices": [{"id": 1639, "choice": " Local SSD\n", "answer": 0}, {"id": 1640, "choice": "Standard Persistent Disk\n", "answer": 0}, {"id": 1641, "choice": "SSD Persistent Disk\n", "answer": 1}, {"id": 1642, "choice": "RAM disk", "answer": 0}]}, {"id": 34, "question": "Which of these statements about Microsoft licenses is true?", "choices": [{"id": 1643, "choice": " You can migrate your existing Microsoft application licenses to Compute Engine instances, but not your\nMicrosoft Windows licenses.\n", "answer": 1}, {"id": 1644, "choice": "You can migrate your existing Microsoft Windows and Microsoft application licenses to Compute Engine\ninstances.\n", "answer": 0}, {"id": 1645, "choice": "You cannot migrate your existing Microsoft Windows or Microsoft application licenses to Compute Engine\ninstances.\n", "answer": 0}, {"id": 1646, "choice": "You can migrate your existing Microsoft Windows licenses to Compute Engine instances, but not your\nMicrosoft application licenses.", "answer": 0}]}, {"id": 35, "question": "Which database services support standard SQL queries?", "choices": [{"id": 1647, "choice": " Cloud Bigtable and Cloud SQL\n", "answer": 0}, {"id": 1648, "choice": "Cloud Spanner and Cloud SQL\n", "answer": 1}, {"id": 1649, "choice": "Cloud SQL and Cloud Datastore\n", "answer": 0}, {"id": 1650, "choice": "Cloud SQL", "answer": 0}]}, {"id": 36, "question": "Which statement about IP addresses is false?", "choices": [{"id": 1651, "choice": " You are charged for a static external IP address for every hour it is in use.\n", "answer": 0}, {"id": 1652, "choice": "You are not charged for ephemeral IP addresses.\n", "answer": 0}, {"id": 1653, "choice": "Google Cloud Engine supports only IPv4 addresses, not IPv6.\n", "answer": 0}, {"id": 1654, "choice": "You are charged for a static external IP address when it is assigned but unused.", "answer": 1}]}, {"id": 37, "question": "Which Google Cloud Platform service requires the least management because it takes care of the underlying\ninfrastructure for you?", "choices": [{"id": 1655, "choice": " Container Engine\n", "answer": 0}, {"id": 1656, "choice": "Cloud Engine\n", "answer": 0}, {"id": 1657, "choice": "App Engine\n", "answer": 1}, {"id": 1658, "choice": "Docker containers running on Cloud Engine", "answer": 0}]}, {"id": 38, "question": "To ensure that your application will handle the load even if an entire zone fails, what should you do?", "choices": [{"id": 1659, "choice": " Don't select the \"Multizone\" option when creating your managed instance group.\n", "answer": 0}, {"id": 1660, "choice": "Spread your managed instance group over two zones and overprovision by 100%.\n", "answer": 0}, {"id": 1661, "choice": "Create a regional unmanaged instance group and spread your instances across multiple zones.\n", "answer": 0}, {"id": 1662, "choice": "Overprovision your regional managed instance group by at least 50%.", "answer": 1}]}, {"id": 39, "question": "If you do not grant a user named Bob permission to access a Cloud Storage bucket, but then use an ACL to\ngrant access to an object inside that bucket to Bob, what will happen?", "choices": [{"id": 1663, "choice": " Bob will be able to access all of the objects inside the bucket because he was granted access to at least\none object in the bucket.\n", "answer": 0}, {"id": 1664, "choice": "Bob will be able to access the object because bucket and object ACLs are independent of each other.\n", "answer": 1}, {"id": 1665, "choice": "Bob will not be able to access the object because he does not have access to the bucket.\n", "answer": 0}, {"id": 1666, "choice": "It is not possible to grant access to an object when it is inside a bucket for which a user does not have\naccess.", "answer": 0}]}, {"id": 40, "question": "To set up a virtual private network between your office network and Google Cloud Platform and have the routes\nautomatically updated when the network topology changes, what is the minimal number of each type of\ncomponent you need to implement?", "choices": [{"id": 1667, "choice": " 2 Cloud VPN Gateways and 1 Peer Gateway\n", "answer": 0}, {"id": 1668, "choice": "1 Cloud VPN Gateway, 1 Peer Gateway, and 1 Cloud Router\n", "answer": 1}, {"id": 1669, "choice": "2 Peer Gateways and 1 Cloud Router\n", "answer": 0}, {"id": 1670, "choice": "2 Cloud VPN Gateways and 1 Cloud Router", "answer": 0}]}, {"id": 41, "question": "Which of the following statements about encryption on GCP is not true?", "choices": [{"id": 1671, "choice": " Google Cloud Platform encrypts customer data stored at rest by default.\n", "answer": 0}, {"id": 1672, "choice": "Each encryption key is itself encrypted with a set of master keys.\n", "answer": 0}, {"id": 1673, "choice": "If you want to manage your own encryption keys for data on Google Cloud Storage, the only option is\nCustomer-Managed Encryption Keys (CMEK) using Cloud KMS.\n", "answer": 1}, {"id": 1674, "choice": "Data in Google Cloud Platform is broken into subfile chunks for storage, and each chunk is encrypted at the\nstorage level with an individual encryption key.", "answer": 0}]}, {"id": 42, "question": "Which database service requires that you configure a failover replica to make it highly available?", "choices": [{"id": 1675, "choice": " Cloud Spanner\n", "answer": 0}, {"id": 1676, "choice": "Cloud SQL\n", "answer": 1}, {"id": 1677, "choice": "BigQuery\n", "answer": 0}, {"id": 1678, "choice": "Cloud Datastore", "answer": 0}]}, {"id": 43, "question": "Which of these is not a principle you should apply when setting roles and permissions?", "choices": [{"id": 1679, "choice": " Whenever possible, assign roles to groups instead of to individuals.\n", "answer": 0}, {"id": 1680, "choice": "Grant users the appropriate permissions to facilitate least privilege\n", "answer": 0}, {"id": 1681, "choice": "Whenever possible, assign primitive roles rather than predefined roles.\n", "answer": 1}, {"id": 1682, "choice": "Audit all policy changes by checking the Cloud Audit Logs.", "answer": 0}]}, {"id": 44, "question": "Which of these is not a recommended method of authenticating an application with a Google Cloud service?", "choices": [{"id": 1683, "choice": " Use the gcloud and/or gsutil commands.\n", "answer": 0}, {"id": 1684, "choice": "Request an OAuth2 access token and use it directly.\n", "answer": 0}, {"id": 1685, "choice": "Embed the service account's credentials in the application's source code.\n", "answer": 1}, {"id": 1686, "choice": "Use one of the Google Cloud Client Libraries.", "answer": 0}]}, {"id": 45, "question": "What are two different features that fully isolate groups of VM instances?", "choices": [{"id": 1687, "choice": " Firewall rules and subnetworks\n", "answer": 0}, {"id": 1688, "choice": "Networks and subnetworks\n", "answer": 0}, {"id": 1689, "choice": "Subnetworks and projects\n", "answer": 0}, {"id": 1690, "choice": "Projects and networks", "answer": 1}]}, {"id": 46, "question": "Suppose you have a web server that is working properly, but you can't connect to its instance VM over SSH.\nWhich of these troubleshooting methods can you use without disrupting production traffic? (Select 3 answers.)", "choices": [{"id": 1691, "choice": " Create a snapshot of the disk and use it to create a new disk; then attach the new disk to a new instance\n", "answer": 1}, {"id": 1692, "choice": "Use netcat to try to connect to port 22\n", "answer": 1}, {"id": 1693, "choice": "Access the serial console output\n", "answer": 1}, {"id": 1694, "choice": "Create a startup script to collect information.", "answer": 0}]}, {"id": 47, "question": "To configure Stackdriver to monitor a web server and let you know if it goes down, what steps do you need to\ntake? (Select 2 answers.)", "choices": [{"id": 1695, "choice": " Install the Stackdriver Logging Agent on the web server\n", "answer": 0}, {"id": 1696, "choice": "Create an alerting policy\n", "answer": 1}, {"id": 1697, "choice": "Install the Stackdriver Monitoring Agent on the web server\n", "answer": 0}, {"id": 1698, "choice": "Create an uptime check", "answer": 1}]}, {"id": 48, "question": "Which of these tools can you use to copy data from AWS S3 to Cloud Storage? (Select 2 answers.)", "choices": [{"id": 1699, "choice": " Cloud Storage Transfer Service\n", "answer": 1}, {"id": 1700, "choice": "S3 Storage Transfer Service\n", "answer": 0}, {"id": 1701, "choice": "Cloud Storage Console\n", "answer": 0}, {"id": 1702, "choice": "gsutil", "answer": 1}]}, {"id": 49, "question": "What are two of the actions you can take to troubleshoot a virtual machine instance that won't start up at all?\n(Select 2 answers.)", "choices": [{"id": 1703, "choice": " Increase the CPU and memory on the instance by changing the machine type.\n", "answer": 0}, {"id": 1704, "choice": "Validate that your disk has a valid file system.\n", "answer": 1}, {"id": 1705, "choice": "Examine your virtual machine instance's serial port output.\n", "answer": 1}, {"id": 1706, "choice": "Connect to your virtual machine instance using SSH.", "answer": 0}]}, {"id": 50, "question": "Which statements about application load testing are true? (Select 2 answers.)", "choices": [{"id": 1707, "choice": " You should test at the maximum load that you expect to encounter.\n", "answer": 1}, {"id": 1708, "choice": "You should test at 50% more than the maximum load that you expect to encounter.\n", "answer": 0}, {"id": 1709, "choice": "It is not necessary to test sudden increases in traffic since GCP scales seamlessly.\n", "answer": 0}, {"id": 1710, "choice": "Your load tests should include testing sudden increases in traffic.", "answer": 1}]}, {"id": 51, "question": "Which of these statements about resilience testing are true? (Select 2 answers.)", "choices": [{"id": 1711, "choice": " In a resilience test, your application should keep running with little or no downtime.\n", "answer": 1}, {"id": 1712, "choice": "To test the resilience of an autoscaling instance group, you can terminate a random instance within that\ngroup.\n", "answer": 1}, {"id": 1713, "choice": "In order for an application to survive instance failures, it should not be stateless.\n", "answer": 0}, {"id": 1714, "choice": "Resilience testing is the same as disaster recovery testing.", "answer": 0}]}, {"id": 52, "question": "Which combination of Stackdriver services will alert you about errors generated by your applications and help\nyou locate the root cause in the code?", "choices": [{"id": 1715, "choice": " Monitoring, Trace, and Debugger\n", "answer": 0}, {"id": 1716, "choice": "Monitoring and Error Reporting\n", "answer": 0}, {"id": 1717, "choice": "Debugger and Error Reporting\n", "answer": 1}, {"id": 1718, "choice": "Alerts and Debugger", "answer": 0}]}, {"id": 53, "question": "If you have configured Stackdriver Logging to export logs to BigQuery, but logs entries are not getting exported\nto BigQuery, what is the most likely cause?", "choices": [{"id": 1719, "choice": " The Cloud Data Transfer Service has not been enabled.\n", "answer": 0}, {"id": 1720, "choice": "There isn't a firewall rule allowing traffic between Stackdriver and BigQuery.\n", "answer": 0}, {"id": 1721, "choice": "Stackdriver Logging does not have permission to write to the BigQuery dataset.\n", "answer": 1}, {"id": 1722, "choice": "The size of the Stackdriver log entries being exported exceeds the maximum capacity of the BigQuery\ndataset.", "answer": 0}]}, {"id": 54, "question": "You can use Stackdriver to monitor virtual machines on which cloud platforms?", "choices": [{"id": 1723, "choice": " Google Cloud Platform, Microsoft Azure\n", "answer": 0}, {"id": 1724, "choice": "Google Cloud Platform\n", "answer": 0}, {"id": 1725, "choice": "Google Cloud Platform, Microsoft Azure, Amazon Web Services\n", "answer": 0}, {"id": 1726, "choice": "Google Cloud Platform, Amazon Web Services", "answer": 1}]}, {"id": 55, "question": "To minimize the risk of someone changing your log files to hide their activities, which of the following principles\nwould help? (Select 3 answers.)", "choices": [{"id": 1727, "choice": " Restrict usage of the owner role for projects and log buckets.\n", "answer": 1}, {"id": 1728, "choice": "Require two people to inspect the logs.\n", "answer": 1}, {"id": 1729, "choice": "Implement object versioning on the log-buckets.\n", "answer": 1}, {"id": 1730, "choice": "Encrypt the logs using Cloud KMS.", "answer": 0}]}, {"id": 56, "question": "If network traffic between one Google Compute Engine instance and another instance is being dropped, what is\nthe most likely cause?", "choices": [{"id": 1731, "choice": " The instances are on a network with low bandwidth.\n", "answer": 0}, {"id": 1732, "choice": "The TCP keep-alive setting is too short.\n", "answer": 0}, {"id": 1733, "choice": "The instances are on a default network with no additional firewall rules.\n", "answer": 0}, {"id": 1734, "choice": "A firewall rule was deleted.", "answer": 1}]}, {"id": 57, "question": "Which of the following practices can help you develop more secure software? (Select 3 answers.)", "choices": [{"id": 1735, "choice": " Penetration tests\n", "answer": 1}, {"id": 1736, "choice": "Integrating static code analysis tools into your CI/CD pipeline\n", "answer": 1}, {"id": 1737, "choice": "Encrypting your source code\n", "answer": 0}, {"id": 1738, "choice": "Peer review of code", "answer": 1}]}, {"id": 58, "question": "Which two places hold information you can use to monitor the effects of a Cloud Storage lifecycle policy on\nspecific objects? (Select 2 answers.)", "choices": [{"id": 1739, "choice": " Cloud Storage Lifecycle Monitoring\n", "answer": 0}, {"id": 1740, "choice": "Expiration time metadata\n", "answer": 1}, {"id": 1741, "choice": "Access logs\n", "answer": 1}, {"id": 1742, "choice": "Lifecycle config file", "answer": 0}]}, {"id": 59, "question": "If you have object versioning enabled on a multi-regional bucket, what will the following lifecycle config file do?\n{\"lifecycle\": {\n\"rule\": [ {\n\"action\": {\"type\": \"Delete\"},\n\"condition\": {\n\"age\": 30,\n\"isLive\": true } },\n{ \"action\": {\n\"type\": \"SetStorageClass\",\n\"storageClass\": \"COLDLINE\"},\n\"condition\": {\n\"age\": 365,\n\"matchesStorageClass\": [\"MULTI_REGIONAL\"] } } ] } }", "choices": [{"id": 1743, "choice": " Archive objects older than 30 days (the second rule doesn't do anything)\n", "answer": 0}, {"id": 1744, "choice": "Delete objects older than 30 days (the second rule doesn't do anything)\n", "answer": 0}, {"id": 1745, "choice": "Archive objects older than 30 days and move objects to Coldline Storage after 365 days\n", "answer": 1}, {"id": 1746, "choice": "Delete objects older than 30 days and move objects to Coldline Storage after 365 days", "answer": 0}]}, {"id": 60, "question": "Which of the following statements about Stackdriver Trace are true? (Select 2 answers.)", "choices": [{"id": 1747, "choice": " Stackdriver Trace tracks the performance of the virtual machines running the application.\n", "answer": 0}, {"id": 1748, "choice": "Stackdriver Trace tracks the latency of incoming requests.\n", "answer": 1}, {"id": 1749, "choice": "Applications in App Engine automatically submit traces to Stackdriver Trace. Applications outside of App\nEngine need to use the Trace SDK or Trace API.\n", "answer": 1}, {"id": 1750, "choice": "To make an application work with Stackdriver Trace, you need to add instrumentation code using the Trace\nSDK or Trace API, even if the application is in App Engine.", "answer": 0}]}, {"id": 61, "question": "Your company\u2019s test suite is a custom C++ application that runs tests throughout each day on Linux virtual\nmachines. The full test suite takes several hours to complete, running on a limited number of on-premises\nservers reserved for testing. Your company wants to move the testing infrastructure to the cloud, to reduce the\namount of time it takes to fully test a change to the system, while changing the tests as little as possible.\nWhich cloud infrastructure should you recommend?", "choices": [{"id": 1751, "choice": " Google Compute Engine unmanaged instance groups and Network Load Balancer\n", "answer": 0}, {"id": 1752, "choice": "Google Compute Engine managed instance groups with auto-scaling\n", "answer": 1}, {"id": 1753, "choice": "Google Cloud Dataproc to run Apache Hadoop jobs to process each test\n", "answer": 0}, {"id": 1754, "choice": "Google App Engine with Google StackDriver for logging", "answer": 0}]}, {"id": 62, "question": "A lead software engineer tells you that his new application design uses websockets and HTTP sessions that\nare not distributed across the web servers. You want to help him ensure his application will run properly on\nGoogle Cloud Platform.\nWhat should you do?", "choices": [{"id": 1755, "choice": " Help the engineer to convert his websocket code to use HTTP streaming\n", "answer": 0}, {"id": 1756, "choice": "Review the encryption requirements for websocket connections with the security team\n", "answer": 0}, {"id": 1757, "choice": "Meet with the cloud operations team and the engineer to discuss load balancer options\n", "answer": 1}, {"id": 1758, "choice": "Help the engineer redesign the application to use a distributed user session service that does not rely on\nwebsockets and HTTP sessions.", "answer": 0}]}, {"id": 63, "question": "The application reliability team at your company this added a debug feature to their backend service to send all\nserver events to Google Cloud Storage for eventual analysis. The event records are at least 50 KB and at most\n15 MB and are expected to peak at 3,000 events per second. You want to minimize data loss.\nWhich process should you implement?", "choices": [{"id": 1759, "choice": " \u2022 Append metadata to file body\n\u2022 Compress individual files\n\u2022 Name files with serverName \u2013 Timestamp\n\u2022 Create a new bucket if bucket is older than 1 hour and save individual files to the new bucket. Otherwise,\nsave files to existing bucket.\n", "answer": 1}, {"id": 1760, "choice": "\u2022 Batch every 10,000 events with a single manifest file for metadata\n\u2022 Compress event files and manifest file into a single archive file\n\u2022 Name files using serverName \u2013 EventSequence\n\u2022 Create a new bucket if bucket is older than 1 day and save the single archive file to the new bucket.\nOtherwise, save the single archive file to existing bucket.\n", "answer": 0}, {"id": 1761, "choice": "\u2022 Compress individual files\n\u2022 Name files with serverName \u2013 EventSequence\n\u2022 Save files to one bucket\n\u2022 Set custom metadata headers for each object after saving\n", "answer": 0}, {"id": 1762, "choice": "\u2022 Append metadata to file body\n\u2022 Compress individual files\n\u2022 Name files with a random prefix pattern\n\u2022 Save files to one bucket", "answer": 0}]}, {"id": 64, "question": "A recent audit revealed that a new network was created in your GCP project. In this network, a GCE instance\nhas an SSH port open to the world. You want to discover this network\u2019s origin.\nWhat should you do?", "choices": [{"id": 1763, "choice": " Search for Create VM entry in the Stackdriver alerting console\n", "answer": 0}, {"id": 1764, "choice": "Navigate to the Activity page in the Home section. Set category to Data Access and search for Create VM\nentry\n", "answer": 0}, {"id": 1765, "choice": "In the Logging section of the console, specify GCE Network as the logging section. Search for the Create\nInsert entry\n", "answer": 1}, {"id": 1766, "choice": "Connect to the GCE instance using project SSH keys. Identify previous logins in system logs, and match\nthese with the project owners list", "answer": 0}]}, {"id": 65, "question": "You want to make a copy of a production Linux virtual machine in the US-Central region. You want to manage\nand replace the copy easily if there are changes on the production virtual machine. You will deploy the copy as\na new instance in a different project in the US-East region.\nWhat steps must you take?", "choices": [{"id": 1767, "choice": " Use the Linux dd and netcat commands to copy and stream the root disk contents to a new virtual machine\ninstance in the US-East region.\n", "answer": 0}, {"id": 1768, "choice": "Create a snapshot of the root disk and select the snapshot as the root disk when you create a new virtual\nmachine instance in the US-East region.\n", "answer": 0}, {"id": 1769, "choice": "Create an image file from the root disk with Linux dd command, create a new virtual machine instance in\nthe US-East region\n", "answer": 0}, {"id": 1770, "choice": "Create a snapshot of the root disk, create an image file in Google Cloud Storage from the snapshot, and\ncreate a new virtual machine instance in the US-East region using the image file the root disk.", "answer": 1}]}, {"id": 66, "question": "Your company runs several databases on a single MySQL instance. They need to take backups of a specific\ndatabase at regular intervals. The backup activity needs to complete as quickly as possible and cannot be\nallowed to impact disk performance.\nHow should you configure the storage?", "choices": [{"id": 1771, "choice": " Configure a cron job to use the gcloud tool to take regular backups using persistent disk snapshots.\n", "answer": 0}, {"id": 1772, "choice": "Mount a Local SSD volume as the backup location. After the backup is complete, use gsutil to move the\nbackup to Google Cloud Storage.\n", "answer": 0}, {"id": 1773, "choice": "Use gcsfise to mount a Google Cloud Storage bucket as a volume directly on the instance and write\nbackups to the mounted location using mysqldump.\n", "answer": 1}, {"id": 1774, "choice": "Mount additional persistent disk volumes onto each virtual machine (VM) instance in a RAID10 array and\nuse LVM to create snapshots to send to Cloud Storage", "answer": 0}]}, {"id": 67, "question": "You are helping the QA team to roll out a new load-testing tool to test the scalability of your primary cloud\nservices that run on Google Compute Engine with Cloud Bigtable.\nWhich three requirements should they include? Choose 3 answers.", "choices": [{"id": 1775, "choice": " Ensure that the load tests validate the performance of Cloud Bigtable\n", "answer": 0}, {"id": 1776, "choice": "Create a separate Google Cloud project to use for the load-testing environment\n", "answer": 1}, {"id": 1777, "choice": "Schedule the load-testing tool to regularly run against the production environment\n", "answer": 0}, {"id": 1778, "choice": "Ensure all third-party systems your services use is capable of handling high load\n", "answer": 0}, {"id": 1779, "choice": "Instrument the production services to record every transaction for replay by the load-testing tool\n", "answer": 1}, {"id": 1780, "choice": "Instrument the load-testing tool and the target services with detailed logging and metrics collection", "answer": 1}]}, {"id": 68, "question": "Your customer is moving their corporate applications to Google Cloud Platform. The security team wants\ndetailed visibility of all projects in the organization. You provision the Google Cloud Resource Manager and set\nup yourself as the org admin.\nWhat Google Cloud Identity and Access Management (Cloud IAM) roles should you give to the security team?", "choices": [{"id": 1781, "choice": " Org viewer, project owner\n", "answer": 0}, {"id": 1782, "choice": "Org viewer, project viewer\n", "answer": 1}, {"id": 1783, "choice": "Org admin, project browser\n", "answer": 0}, {"id": 1784, "choice": "Project owner, network admin", "answer": 0}]}, {"id": 69, "question": "Your company places a high value on being responsive and meeting customer needs quickly. Their primary\nbusiness objectives are release speed and agility. You want to reduce the chance of security errors being\naccidentally introduced.\nWhich two actions can you take? Choose 2 answers.", "choices": [{"id": 1785, "choice": " Ensure every code check-in is peer reviewed by a security SME\n", "answer": 0}, {"id": 1786, "choice": "Use source code security analyzers as part of the CI/CD pipeline\n", "answer": 1}, {"id": 1787, "choice": "Ensure you have stubs to unit test all interfaces between components\n", "answer": 0}, {"id": 1788, "choice": "Enable code signing and a trusted binary repository integrated with your CI/CD pipeline\n", "answer": 0}, {"id": 1789, "choice": "Run a vulnerability security scanner as part of your continuous-integration /continuous-delivery (CI/CD)\npipeline", "answer": 1}]}, {"id": 70, "question": "You want to enable your running Google Kubernetes Engine cluster to scale as demand for your application\nchanges.\nWhat should you do?", "choices": [{"id": 1790, "choice": " Add additional nodes to your Kubernetes Engine cluster using the following command:\ngcloud container clusters resize\nCLUSTER_Name \u2013 -size 10\n", "answer": 0}, {"id": 1791, "choice": "Add a tag to the instances in the cluster with the following command:\ngcloud compute instances add-tags\nINSTANCE - -tags enable-\nautoscaling max-nodes-10\n", "answer": 1}, {"id": 1792, "choice": "Update the existing Kubernetes Engine cluster with the following command:\ngcloud alpha container clusters\nupdate mycluster - -enable-\nautoscaling - -min-nodes=1 - -max-nodes=10\n", "answer": 0}, {"id": 1793, "choice": "Create a new Kubernetes Engine cluster with the following command:\ngcloud alpha container clusters\ncreate mycluster - -enable-\nautoscaling - -min-nodes=1 - -max-nodes=10\nand redeploy your application", "answer": 0}]}, {"id": 71, "question": "Your marketing department wants to send out a promotional email campaign. The development team wants to\nminimize direct operation management. They project a wide range of possible customer responses, from 100 to\n500,000 click-through per day. The link leads to a simple website that explains the promotion and collects user\ninformation and preferences.\nWhich infrastructure should you recommend? Choose 2 answers.", "choices": [{"id": 1794, "choice": " Use Google App Engine to serve the website and Google Cloud Datastore to store user data.\n", "answer": 1}, {"id": 1795, "choice": "Use a Google Container Engine cluster to serve the website and store data to persistent disk.\n", "answer": 0}, {"id": 1796, "choice": "Use a managed instance group to serve the website and Google Cloud Bigtable to store user data.\n", "answer": 1}, {"id": 1797, "choice": "Use a single Compute Engine virtual machine (VM) to host a web server, backend by Google Cloud SQL.", "answer": 0}]}, {"id": 72, "question": "Your company just finished a rapid lift and shift to Google Compute Engine for your compute needs. You have\nanother 9 months to design and deploy a more cloud-native solution. Specifically, you want a system that is no-\nops and auto-scaling.\nWhich two compute products should you choose? Choose 2 answers.", "choices": [{"id": 1798, "choice": " Compute Engine with containers\n", "answer": 0}, {"id": 1799, "choice": "Google Kubernetes Engine with containers\n", "answer": 1}, {"id": 1800, "choice": "Google App Engine Standard Environment\n", "answer": 1}, {"id": 1801, "choice": "Compute Engine with custom instance types\n", "answer": 0}, {"id": 1802, "choice": "Compute Engine with managed instance groups", "answer": 0}]}, {"id": 73, "question": "One of your primary business objectives is being able to trust the data stored in your application. You want to\nlog all changes to the application data.\nHow can you design your logging system to verify authenticity of your logs?", "choices": [{"id": 1803, "choice": " Write the log concurrently in the cloud and on premises\n", "answer": 0}, {"id": 1804, "choice": "Use a SQL database and limit who can modify the log table\n", "answer": 0}, {"id": 1805, "choice": "Digitally sign each timestamp and log entry and store the signature\n", "answer": 0}, {"id": 1806, "choice": "Create a JSON dump of each log entry and store it in Google Cloud Storage", "answer": 1}]}, {"id": 74, "question": "Your company has decided to make a major revision of their API in order to create better experiences for their\ndevelopers. They need to keep the old version of the API available and deployable, while allowing new\ncustomers and testers to try out the new API. They want to keep the same SSL and DNS records in place to\nserve both APIs.\nWhat should they do?", "choices": [{"id": 1807, "choice": " Configure a new load balancer for the new version of the API\n", "answer": 0}, {"id": 1808, "choice": "Reconfigure old clients to use a new endpoint for the new API\n", "answer": 0}, {"id": 1809, "choice": "Have the old API forward traffic to the new API based on the path\n", "answer": 0}, {"id": 1810, "choice": "Use separate backend pools for each API path behind the load balancer", "answer": 1}]}, {"id": 75, "question": "Your company plans to migrate a multi-petabyte data set to the cloud. The data set must be available 24hrs a\nday. Your business analysts have experience only with using a SQL interface.\nHow should you store the data to optimize it for ease of analysis?", "choices": [{"id": 1811, "choice": " Load data into Google BigQuery\n", "answer": 1}, {"id": 1812, "choice": "Insert data into Google Cloud SQL\n", "answer": 0}, {"id": 1813, "choice": "Put flat files into Google Cloud Storage\n", "answer": 0}, {"id": 1814, "choice": "Stream data into Google Cloud Datastore", "answer": 0}]}, {"id": 76, "question": "The operations manager asks you for a list of recommended practices that she should consider when migrating\na J2EE application to the cloud.\nWhich three practices should you recommend? Choose 3 answers.", "choices": [{"id": 1815, "choice": " Port the application code to run on Google App Engine\n", "answer": 1}, {"id": 1816, "choice": "Integrate Cloud Dataflow into the application to capture real-time metrics\n", "answer": 0}, {"id": 1817, "choice": "Instrument the application with a monitoring tool like Stackdriver Debugger\n", "answer": 0}, {"id": 1818, "choice": "Select an automation framework to reliably provision the cloud infrastructure\n", "answer": 1}, {"id": 1819, "choice": "Deploy a continuous integration tool with automated testing in a staging environment\n", "answer": 1}, {"id": 1820, "choice": "Migrate from MySQL to a managed NoSQL database like Google Cloud Datastore or Bigtable", "answer": 0}]}, {"id": 77, "question": "A news feed web service has the following code running on Google App Engine. During peak load, users report\nthat they can see news articles they already viewed.\nWhat is the most likely cause of this problem?", "choices": [{"id": 1821, "choice": " The session variable is local to just a single instance\n", "answer": 0}, {"id": 1822, "choice": "The session variable is being overwritten in Cloud Datastore\n", "answer": 1}, {"id": 1823, "choice": "The URL of the API needs to be modified to prevent caching\n", "answer": 0}, {"id": 1824, "choice": "The HTTP Expires header needs to be set to -1 stop caching", "answer": 0}]}, {"id": 78, "question": "An application development team believes their current logging tool will not meet their needs for their new\ncloud-based product. They want a better tool to capture errors and help them analyze their historical log data.\nYou want to help them find a solution that meets their needs.\nWhat should you do?", "choices": [{"id": 1825, "choice": " Direct them to download and install the Google StackDriver logging agent\n", "answer": 1}, {"id": 1826, "choice": "Send them a list of online resources about logging best practices\n", "answer": 0}, {"id": 1827, "choice": "Help them define their requirements and assess viable logging tools\n", "answer": 0}, {"id": 1828, "choice": "Help them upgrade their current tool to take advantage of any new features", "answer": 0}]}, {"id": 79, "question": "You need to reduce the number of unplanned rollbacks of erroneous production deployments in your\ncompany\u2019s web hosting platform. Improvement to the QA/Test processes accomplished an 80% reduction.\nWhich additional two approaches can you take to further reduce the rollbacks? Choose 2 answers.", "choices": [{"id": 1829, "choice": " Introduce a green-blue deployment model\n", "answer": 1}, {"id": 1830, "choice": "Replace the QA environment with canary releases\n", "answer": 0}, {"id": 1831, "choice": "Fragment the monolithic platform into microservices\n", "answer": 1}, {"id": 1832, "choice": "Reduce the platform\u2019s dependency on relational database systems\n", "answer": 0}, {"id": 1833, "choice": "Replace the platform\u2019s relational database systems with a NoSQL database", "answer": 0}]}, {"id": 80, "question": "To reduce costs, the Director of Engineering has required all developers to move their development\ninfrastructure resources from on-premises virtual machines (VMs) to Google Cloud Platform. These resources\ngo through multiple start/stop events during the day and require state to persist. You have been asked to design\nthe process of running a development environment in Google Cloud while providing cost visibility to the finance\ndepartment.\nWhich two steps should you take? Choose 2 answers.", "choices": [{"id": 1834, "choice": " Use the - -no-auto-delete flag on all persistent disks and stop the VM\n", "answer": 0}, {"id": 1835, "choice": "Use the - -auto-delete flag on all persistent disks and terminate the VM\n", "answer": 0}, {"id": 1836, "choice": "Apply VM CPU utilization label and include it in the BigQuery billing export\n", "answer": 1}, {"id": 1837, "choice": "Use Google BigQuery billing export and labels to associate cost to groups\n", "answer": 0}, {"id": 1838, "choice": "Store all state into local SSD, snapshot the persistent disks, and terminate the VM\n", "answer": 1}, {"id": 1839, "choice": "Store all state in Google Cloud Storage, snapshot the persistent disks, and terminate the VM", "answer": 0}]}, {"id": 81, "question": "Your company wants to track whether someone is present in a meeting room reserved for a scheduled\nmeeting. There are 1000 meeting rooms across 5 offices on 3 continents. Each room is equipped with a motion\nsensor that reports its status every second. The data from the motion detector includes only a sensor ID and\nseveral different discrete items of information. Analysts will use this data, together with information about\naccount owners and office locations.\nWhich database type should you use?", "choices": [{"id": 1840, "choice": " Flat file\n", "answer": 0}, {"id": 1841, "choice": "NoSQL\n", "answer": 1}, {"id": 1842, "choice": "Relational\n", "answer": 0}, {"id": 1843, "choice": "Blobstore", "answer": 0}]}, {"id": 82, "question": "You set up an autoscaling instance group to serve web traffic for an upcoming launch. After configuring the\ninstance group as a backend service to an HTTP(S) load balancer, you notice that virtual machine (VM)\ninstances are being terminated and re-launched every minute. The instances do not have a public IP address.\nYou have verified the appropriate web response is coming from each instance using the curl command. You\nwant to ensure the backend is configured correctly.\nWhat should you do?", "choices": [{"id": 1844, "choice": " Ensure that a firewall rules exists to allow source traffic on HTTP/HTTPS to reach the load balancer.\n", "answer": 0}, {"id": 1845, "choice": "Assign a public IP to each instance and configure a firewall rule to allow the load balancer to reach the\ninstance public IP.\n", "answer": 0}, {"id": 1846, "choice": "Ensure that a firewall rule exists to allow load balancer health checks to reach the instances in the instance\ngroup.\n", "answer": 1}, {"id": 1847, "choice": "Create a tag on each instance with the name of the load balancer. Configure a firewall rule with the name of\nthe load balancer as the source and the instance tag as the destination.", "answer": 0}]}, {"id": 83, "question": "You write a Python script to connect to Google BigQuery from a Google Compute Engine virtual machine. The\nscript is printing errors that it cannot connect to BigQuery.\nWhat should you do to fix the script?", "choices": [{"id": 1848, "choice": " Install the latest BigQuery API client library for Python\n", "answer": 1}, {"id": 1849, "choice": "Run your script on a new virtual machine with the BigQuery access scope enabled\n", "answer": 0}, {"id": 1850, "choice": "Create a new service account with BigQuery access and execute your script with that user\n", "answer": 0}, {"id": 1851, "choice": "Install the bq component for gcloud with the command gcloud components install bq.", "answer": 0}]}, {"id": 84, "question": "Your customer is moving an existing corporate application to Google Cloud Platform from an on-premises data\ncenter. The business owners require minimal user disruption. There are strict security team requirements for\nstoring passwords.\nWhat authentication strategy should they use?", "choices": [{"id": 1852, "choice": " Use G Suite Password Sync to replicate passwords into Google\n", "answer": 0}, {"id": 1853, "choice": "Federate authentication via SAML 2.0 to the existing Identity Provider\n", "answer": 0}, {"id": 1854, "choice": "Provision users in Google using the Google Cloud Directory Sync tool\n", "answer": 1}, {"id": 1855, "choice": "Ask users to set their Google password to match their corporate password", "answer": 0}]}, {"id": 85, "question": "Your company has successfully migrated to the cloud and wants to analyze their data stream to optimize\noperations. They do not have any existing code for this analysis, so they are exploring all their options. These\noptions include a mix of batch and stream processing, as they are running some hourly jobs and live-\nprocessing some data as it comes in.\nWhich technology should they use for this?", "choices": [{"id": 1856, "choice": " Google Cloud Dataproc\n", "answer": 0}, {"id": 1857, "choice": "Google Cloud Dataflow\n", "answer": 1}, {"id": 1858, "choice": "Google Container Engine with Bigtable\n", "answer": 0}, {"id": 1859, "choice": "Google Compute Engine with Google BigQuery", "answer": 0}]}, {"id": 86, "question": "Your customer is receiving reports that their recently updated Google App Engine application is taking\napproximately 30 seconds to load for some of their users. This behavior was not reported before the update.\nWhat strategy should you take?", "choices": [{"id": 1860, "choice": " Work with your ISP to diagnose the problem\n", "answer": 0}, {"id": 1861, "choice": "Open a support ticket to ask for network capture and flow data to diagnose the problem, then roll back your\napplication\n", "answer": 0}, {"id": 1862, "choice": "Roll back to an earlier known good release initially, then use Stackdriver Trace and Logging to diagnose the\nproblem in a development/test/staging environment\n", "answer": 1}, {"id": 1863, "choice": "Roll back to an earlier known good release, then push the release again at a quieter period to investigate.\nThen use Stackdriver Trace and Logging to diagnose the problem", "answer": 0}]}, {"id": 87, "question": "A production database virtual machine on Google Compute Engine has an ext4-formatted persistent disk for\ndata files. The database is about to run out of storage space.\nHow can you remediate the problem with the least amount of downtime?", "choices": [{"id": 1864, "choice": " In the Cloud Platform Console, increase the size of the persistent disk and use the resize2fs command in\nLinux.\n", "answer": 1}, {"id": 1865, "choice": "Shut down the virtual machine, use the Cloud Platform Console to increase the persistent disk size, then\nrestart the virtual machine\n", "answer": 0}, {"id": 1866, "choice": "In the Cloud Platform Console, increase the size of the persistent disk and verify the new space is ready to\nuse with the fdisk command in Linux\n", "answer": 0}, {"id": 1867, "choice": "In the Cloud Platform Console, create a new persistent disk attached to the virtual machine, format and\nmount it, and configure the database service to move the files to the new disk\n", "answer": 0}, {"id": 1868, "choice": "In the Cloud Platform Console, create a snapshot of the persistent disk restore the snapshot to a new larger\ndisk, unmount the old disk, mount the new disk and restart the database service", "answer": 0}]}, {"id": 88, "question": "Your application needs to process credit card transactions. You want the smallest scope of Payment Card\nIndustry (PCI) compliance without compromising the ability to analyze transactional data and trends relating to\nwhich payment methods are used.\nHow should you design your architecture?", "choices": [{"id": 1869, "choice": " Create a tokenizer service and store only tokenized data\n", "answer": 1}, {"id": 1870, "choice": "Create separate projects that only process credit card data\n", "answer": 0}, {"id": 1871, "choice": "Create separate subnetworks and isolate the components that process credit card data\n", "answer": 0}, {"id": 1872, "choice": "Streamline the audit discovery phase by labeling all of the virtual machines (VMs) that process PCI data\n", "answer": 0}, {"id": 1873, "choice": "Enable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the\nauditor", "answer": 0}]}, {"id": 89, "question": "You have been asked to select the storage system for the click-data of your company\u2019s large portfolio of\nwebsites. This data is streamed in from a custom website analytics package at a typical rate of 6,000 clicks per\nminute. With bursts of up to 8,500 clicks per second. It must have been stored for future analysis by your data\nscience and user experience teams.\nWhich storage infrastructure should you choose?", "choices": [{"id": 1874, "choice": " Google Cloud SQL\n", "answer": 0}, {"id": 1875, "choice": "Google Cloud Bigtable\n", "answer": 1}, {"id": 1876, "choice": "Google Cloud Storage\n", "answer": 0}, {"id": 1877, "choice": "Google Cloud Datastore", "answer": 0}]}, {"id": 90, "question": "You are creating a solution to remove backup files older than 90 days from your backup Cloud Storage bucket.\nYou want to optimize ongoing Cloud Storage spend.\nWhat should you do?", "choices": [{"id": 1878, "choice": " Write a lifecycle management rule in XML and push it to the bucket with gsutil\n", "answer": 0}, {"id": 1879, "choice": "Write a lifecycle management rule in JSON and push it to the bucket with gsutil\n", "answer": 1}, {"id": 1880, "choice": "Schedule a cron script using gsutil ls \u2013lr gs://backups/** to find and remove items older than 90\ndays\n", "answer": 0}, {"id": 1881, "choice": "Schedule a cron script using gsutil ls \u2013l gs://backups/** to find and remove items older than 90\ndays and schedule it with cron", "answer": 0}]}, {"id": 91, "question": "Your company is forecasting a sharp increase in the number and size of Apache Spark and Hadoop jobs being\nrun on your local datacenter. You want to utilize the cloud to help you scale this upcoming demand with the\nleast amount of operations work and code change.\nWhich product should you use?", "choices": [{"id": 1882, "choice": " Google Cloud Dataflow\n", "answer": 0}, {"id": 1883, "choice": "Google Cloud Dataproc\n", "answer": 1}, {"id": 1884, "choice": "Google Compute Engine\n", "answer": 0}, {"id": 1885, "choice": "Google Kubernetes Engine", "answer": 0}]}, {"id": 92, "question": "The database administration team has asked you to help them improve the performance of their new database\nserver running on Google Compute Engine. The database is for importing and normalizing their performance\nstatistics and is built with MySQL running on Debian Linux. They have an n1-standard-8 virtual machine with 80\nGB of SSD persistent disk.\nWhat should they change to get better performance from this system?", "choices": [{"id": 1886, "choice": " Increase the virtual machine\u2019s memory to 64 GB\n", "answer": 0}, {"id": 1887, "choice": "Create a new virtual machine running PostgreSQL\n", "answer": 0}, {"id": 1888, "choice": "Dynamically resize the SSD persistent disk to 500 GB\n", "answer": 1}, {"id": 1889, "choice": "Migrate their performance metrics warehouse to BigQuery\n", "answer": 0}, {"id": 1890, "choice": "Modify all of their batch jobs to use bulk inserts into the database", "answer": 0}]}, {"id": 93, "question": "You want to optimize the performance of an accurate, real-time, weather-charting application. The data comes\nfrom 50,000 sensors sending 10 readings a second, in the format of a timestamp and sensor reading.\nWhere should you store the data?", "choices": [{"id": 1891, "choice": "\nB.\nC.\nD.\nGoogle BigQuery\nGoogle Cloud SQL\nGoogle Cloud Bigtable\nGoogle Cloud Storage", "answer": 0}]}, {"id": 94, "question": "Your company\u2019s user-feedback portal comprises a standard LAMP stack replicated across two zones. It is\ndeployed in the us-central1 region and uses autoscaled managed instance groups on all layers, except the\ndatabase. Currently, only a small group of select customers have access to the portal. The portal meets a\n99,99% availability SLA under these conditions. However next quarter, your company will be making the portal\navailable to all users, including unauthenticated users. You need to develop a resiliency testing strategy to\nensure the system maintains the SLA once they introduce additional user load.\nWhat should you do?", "choices": [{"id": 1892, "choice": " Capture existing users input, and replay captured user load until autoscale is triggered on all layers. At the\nsame time, terminate all resources in one of the zones\n", "answer": 0}, {"id": 1893, "choice": "Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one\nlayer, and introduce \u201cchaos\u201d to the system by terminating random resources on both zones\n", "answer": 0}, {"id": 1894, "choice": "Expose the new system to a larger group of users, and increase group size each day until autoscale logic is\ntriggered on all layers. At the same time, terminate random resources on both zones\n", "answer": 0}, {"id": 1895, "choice": "Capture existing users input, and replay captured user load until resource utilization crosses 80%. Also,\nderive estimated number of users based on existing user\u2019s usage of the app, and deploy enough resources\nto handle 200% of expected load", "answer": 1}]}, {"id": 95, "question": "One of the developers on your team deployed their application in Google Container Engine with the Dockerfile\nbelow. They report that their application deployments are taking too long.\nYou want to optimize this Dockerfile for faster deployment times without adversely affecting the app\u2019s\nfunctionality.\nWhich two actions should you take? Choose 2 answers.", "choices": [{"id": 1896, "choice": " Remove Python after running pip\n", "answer": 0}, {"id": 1897, "choice": "Remove dependencies from requirements.txt\n", "answer": 0}, {"id": 1898, "choice": "Use a slimmed-down base image like Alpine Linux\n", "answer": 1}, {"id": 1899, "choice": "Use larger machine types for your Google Container Engine node pools\n", "answer": 0}, {"id": 1900, "choice": "Copy the source after he package dependencies (Python and pip) are installed", "answer": 1}]}, {"id": 96, "question": "Your solution is producing performance bugs in production that you did not see in staging and test\nenvironments. You want to adjust your test and deployment procedures to avoid this problem in the future.\nWhat should you do?", "choices": [{"id": 1901, "choice": " Deploy fewer changes to production\n", "answer": 0}, {"id": 1902, "choice": "Deploy smaller changes to production\n", "answer": 0}, {"id": 1903, "choice": "Increase the load on your test and staging environments\n", "answer": 0}, {"id": 1904, "choice": "Deploy changes to a small subset of users before rolling out to production", "answer": 1}]}, {"id": 98, "question": "During a high traffic portion of the day, one of your relational databases crashes, but the replica is never\npromoted to a master. You want to avoid this in the future.\nWhat should you do?", "choices": [{"id": 1909, "choice": " Use a different database\n", "answer": 0}, {"id": 1910, "choice": "Choose larger instances for your database\n", "answer": 0}, {"id": 1911, "choice": "Create snapshots of your database more regularly\n", "answer": 1}, {"id": 1912, "choice": "Implement routinely scheduled failovers of your databases", "answer": 0}]}, {"id": 99, "question": "Your organization requires that metrics from all applications be retained for 5 years for future analysis in\npossible legal proceedings.\nWhich approach should you use?", "choices": [{"id": 1913, "choice": " Grant the security team access to the logs in each Project\n", "answer": 0}, {"id": 1914, "choice": "Configure Stackdriver Monitoring for all Projects, and export to BigQuery\n", "answer": 1}, {"id": 1915, "choice": "Configure Stackdriver Monitoring for all Projects with the default retention policies\n", "answer": 0}, {"id": 1916, "choice": "Configure Stackdriver Monitoring for all Projects, and export to Google Cloud Storage", "answer": 0}]}, {"id": 100, "question": "Your company has decided to build a backup replica of their on-premises user authentication PostgreSQL\ndatabase on Google Cloud Platform. The database is 4 TB, and large updates are frequent. Replication\nrequires private address space communication.\nWhich networking approach should you use?", "choices": [{"id": 1917, "choice": " Google Cloud Dedicated Interconnect\n", "answer": 1}, {"id": 1918, "choice": "Google Cloud VPN connected to the data center network\n", "answer": 0}, {"id": 1919, "choice": "A NAT and TLS translation gateway installed on-premises\n", "answer": 0}, {"id": 1920, "choice": "A Google Compute Engine instance with a VPN server installed connected to the data center network", "answer": 0}]}, {"id": 101, "question": "Auditors visit your teams every 12 months and ask to review all the Google Cloud Identity and Access\nManagement (Cloud IAM) policy changes in the previous 12 months. You want to streamline and expedite the\nanalysis and audit process.\nWhat should you do?", "choices": [{"id": 1921, "choice": " Create custom Google Stackdriver alerts and send them to the auditor\n", "answer": 0}, {"id": 1922, "choice": "Enable Logging export to Google BigQuery and use ACLs and views to scope the data shared with the\nauditor\n", "answer": 0}, {"id": 1923, "choice": "Use cloud functions to transfer log entries to Google Cloud SQL and use ACLs and views to limit an\nauditor\u2019s view\n", "answer": 0}, {"id": 1924, "choice": "Enable Google Cloud Storage (GCS) log export to audit logs into a GCS bucket and delegate access to the\nbucket", "answer": 1}]}, {"id": 102, "question": "You are designing a large distributed application with 30 microservices. Each of your distributed microservices\nneeds to connect to a database back-end. You want to store the credentials securely.\nWhere should you store the credentials?", "choices": [{"id": 1925, "choice": " In the source code\n", "answer": 0}, {"id": 1926, "choice": "In an environment variable\n", "answer": 0}, {"id": 1927, "choice": "In a secret management system\n", "answer": 1}, {"id": 1928, "choice": "In a config file that has restricted access through ACLs", "answer": 0}]}, {"id": 103, "question": "A lead engineer wrote a custom tool that deploys virtual machines in the legacy data center. He wants to\nmigrate the custom tool to the new cloud environment. You want to advocate for the adoption of Google Cloud\nDeployment Manager.\nWhat are two business risks of migrating to Cloud Deployment Manager? Choose 2 answers.", "choices": [{"id": 1929, "choice": " Cloud Deployment Manager uses Python\n", "answer": 0}, {"id": 1930, "choice": "Cloud Deployment Manager APIs could be deprecated in the future\n", "answer": 1}, {"id": 1931, "choice": "Cloud Deployment Manager is unfamiliar to the company\u2019s engineers\n", "answer": 0}, {"id": 1932, "choice": "Cloud Deployment Manager requires a Google APIs service account to run\n", "answer": 0}, {"id": 1933, "choice": "Cloud Deployment Manager can be used to permanently delete cloud resources\n", "answer": 0}, {"id": 1934, "choice": "Cloud Deployment Manager only supports automation of Google Cloud resources", "answer": 1}]}, {"id": 104, "question": "A development manager is building a new application. He asks you to review his requirements and identify what\ncloud technologies he can use to meet them. The application must:\n1. Be based on open-source technology for cloud portability\n2. Dynamically scale compute capacity based on demand\n3. Support continuous software delivery\n4. Run multiple segregated copies of the same application stack\n5. Deploy application bundles using dynamic templates\n6. Route network traffic to specific services based on URL\n\nWhich combination of technologies will meet all of his requirements?", "choices": [{"id": 1935, "choice": " Google Kubernetes Engine, Jenkins, and Helm\n", "answer": 0}, {"id": 1936, "choice": "Google Kubernetes Engine and Cloud Load Balancing\n", "answer": 0}, {"id": 1937, "choice": "Google Kubernetes Engine and Cloud Deployment Manager\n", "answer": 0}, {"id": 1938, "choice": "Google Kubernetes Engine, Jenkins, and Cloud Load Balancing", "answer": 1}]}, {"id": 105, "question": "You have created several pre-emptible Linux virtual machine instances using Google Compute Engine. You\nwant to properly shut down your application before the virtual machines are preempted.\nWhat should you do?", "choices": [{"id": 1939, "choice": " Create a shutdown script named k99.shutdown in the /etc/rc.6.d/ directory\n", "answer": 0}, {"id": 1940, "choice": "Create a shutdown script registered as a xinetd service in Linux and configure a Stackdriver endpoint\ncheck to call the service\n", "answer": 0}, {"id": 1941, "choice": "Create a shutdown script and use it as the value for a new metadata entry with the key shutdown-script\nin the Cloud Platform Console when you create the new virtual machine instance\n", "answer": 1}, {"id": 1942, "choice": "Create a shutdown script, registered as a xinetd service in Linux, and use the gcloud compute\ninstances add-metadata command to specify the service URL as the value for a new metadata entry\nwith the key shutdown-script-url", "answer": 0}]}, {"id": 106, "question": "Your organization has a 3-tier web application deployed in the same network on Google Cloud Platform. Each\ntier (web, API, and database) scales independently of the others. Network traffic should flow through the web to\nthe API tier and then on to the database tier. Traffic should not flow between the web and the database tier.\nHow should you configure the network?", "choices": [{"id": 1943, "choice": " Add each tier to a different subnetwork\n", "answer": 0}, {"id": 1944, "choice": "Set up software based firewalls on individual VMs\n", "answer": 0}, {"id": 1945, "choice": "Add tags to each tier and set up routes to allow the desired traffic flow\n", "answer": 0}, {"id": 1946, "choice": "Add tags to each tier and set up firewall rules to allow the desired traffic flow", "answer": 1}]}, {"id": 108, "question": "Your company wants to try out the cloud with low risk. They want to archive approximately 100 TB of their log\ndata to the cloud and test the analytics features available to them there, while also retaining that data as a long-\nterm disaster recovery backup.\nWhich two steps should you take? Choose 2 answers.", "choices": [{"id": 1953, "choice": " Load logs into Google BigQuery\n", "answer": 1}, {"id": 1954, "choice": "Load logs into Google Cloud SQL\n", "answer": 0}, {"id": 1955, "choice": "Import logs into Google Stackdriver\n", "answer": 1}, {"id": 1956, "choice": "Insert logs into Google Cloud Bigtable\n", "answer": 0}, {"id": 1957, "choice": "Upload log files into Google Cloud Storage", "answer": 0}]}, {"id": 109, "question": "You created a pipeline that can deploy your source code changes to your infrastructure in instance groups for\nself-healing. One of the changes negatively affects your key performance indicator. You are not sure how to fix\nit, and investigation could take up to a week.\nWhat should you do?", "choices": [{"id": 1958, "choice": " Log in to a server, and iterate on the fox locally\n", "answer": 0}, {"id": 1959, "choice": "Revert the source code change, and rerun the deployment pipeline\n", "answer": 1}, {"id": 1960, "choice": "Log into the servers with the bad code change, and swap in the previous code\n", "answer": 0}, {"id": 1961, "choice": "Change the instance group template to the previous one, and delete all instances", "answer": 0}]}, {"id": 110, "question": "Your organization wants to control IAM policies for different departments independently, but centrally.\nWhich approach should you take?", "choices": [{"id": 1962, "choice": " Multiple Organizations with multiple Folders\n", "answer": 0}, {"id": 1963, "choice": "Multiple Organizations, one for each department\n", "answer": 0}, {"id": 1964, "choice": "A single Organization with Folders for each department\n", "answer": 1}, {"id": 1965, "choice": "A single Organization with multiple projects, each with a central owner", "answer": 0}]}, {"id": 111, "question": "You deploy your custom Java application to Google App Engine. It fails to deploy and gives you the following\nstack trace.\nWhat should you do?", "choices": [{"id": 1966, "choice": " Upload missing JAR files and redeploy your application.\n", "answer": 0}, {"id": 1967, "choice": "Digitally sign all of your JAR files and redeploy your application\n", "answer": 1}, {"id": 1968, "choice": "Recompile the CLoakedServlet class using and MD5 hash instead of SHA1", "answer": 0}]}, {"id": 112, "question": "You are designing a mobile chat application. You want to ensure people cannot spoof chat messages, by\nproviding a message were sent by a specific user.\nWhat should you do?", "choices": [{"id": 1969, "choice": " Tag messages client side with the originating user identifier and the destination user.\n", "answer": 0}, {"id": 1970, "choice": "Encrypt the message client side using block-based encryption with a shared key.\n", "answer": 0}, {"id": 1971, "choice": "Use public key infrastructure (PKI) to encrypt the message client side using the originating user's private\nkey.\n", "answer": 0}, {"id": 1972, "choice": "Use a trusted certificate authority to enable SSL connectivity between the client application and the server.", "answer": 1}]}, {"id": 113, "question": "You want to enable your running Google Container Engine cluster to scale as demand for your application\nchanges.\nWhat should you do?", "choices": [{"id": 1973, "choice": " Add additional nodes to your Container Engine cluster using the following command:\ngcloud container clusters resize\nCLUSTER_Name \u2013 -size 10\n", "answer": 0}, {"id": 1974, "choice": "Add a tag to the instances in the cluster with the following command:\ngcloud compute instances add-tags\nINSTANCE - -tags enable-\nautoscaling max-nodes-10\n", "answer": 1}, {"id": 1975, "choice": "Update the existing Container Engine cluster with the following command:\ngcloud alpha container clusters\nupdate mycluster - -enable-\nautoscaling - -min-nodes=1 - -max-nodes=10\n", "answer": 0}, {"id": 1976, "choice": "Create a new Container Engine cluster with the following command:\ngcloud alpha container clusters\ncreate mycluster - -enable-\nautoscaling - -min-nodes=1 - -max-nodes=10\nand redeploy your application", "answer": 0}]}, {"id": 115, "question": "A few days after JencoMart migrates the user credentials database to Google Cloud Platform and shuts down the old server, the new database server stops responding to SSH connections. It is still serving database requests to the application servers correctly.\nWhat three steps should you take to diagnose the problem? Choose 3 answers.", "choices": [{"id": 1981, "choice": " Deletethevirtualmachine(VM)anddisksandcreateanewone\n", "answer": 0}, {"id": 1982, "choice": "Delete the instance, attach the disk to a new VM, and investigate", "answer": 0}, {"id": 1983, "choice": "Take a snapshot of the disk and connect to a new machine to investigate\n", "answer": 1}, {"id": 1984, "choice": "Check inbound firewall rules for the network the machine is connected to\n", "answer": 1}, {"id": 1985, "choice": "Connect the machine to another network with very simple firewall rules and investigate", "answer": 0}, {"id": 1986, "choice": "PrinttheSerialConsoleoutputfortheinstancefortroubleshooting,activatetheinteractiveconsole,and investigate", "answer": 1}]}, {"id": 116, "question": "JencoMart has decided to migrate user profile storage to Google Cloud Datastore and the application servers to Google Compute Engine (GCE). During the migration, the existing infrastructure will need access to Datastore to upload the data.\nWhat service account key-management strategy should you recommend?", "choices": [{"id": 1987, "choice": " Provisionserviceaccountkeysfortheon-premisesinfrastructureandfortheGCEvirtualmachines(VMs)\n", "answer": 0}, {"id": 1988, "choice": "Authenticatetheon-premisesinfrastructurewithauseraccountandprovisionserviceaccountkeysforthe VMs\n", "answer": 0}, {"id": 1989, "choice": "Provision service account keys for the on-premises infrastructure and use Google Cloud Platform (GCP) managed keys for the VMs\n", "answer": 1}, {"id": 1990, "choice": "Deploy a custom authentication service on GCE/Google Kubernetes Engine (GKE) for the on-premises infrastructure and use GCP managed keys for the VMs", "answer": 0}]}, {"id": 117, "question": "JencoMart has built a version of their application on Google Cloud Platform that serves traffic to Asia. You want to measure success against their business and technical goals.\nWhich metrics should you track?", "choices": [{"id": 1991, "choice": " Error rates for requests from Asi.    . ", "answer": 0}, {"id": 1992, "choice": "LatencydifferencebetweenUSandAsia\n", "answer": 0}, {"id": 1993, "choice": "Total visits, error rates, and latency from Asia\n", "answer": 0}, {"id": 1994, "choice": "Total visits and average latency for users from Asia ", "answer": 1}, {"id": 1995, "choice": "Thenumberofcharactersetspresentinthedatabase", "answer": 0}]}, {"id": 118, "question": "Mountkirk Games has deployed their new backend on Google Cloud Platform (GCP). You want to create a through testing process for new versions of the backend before they are released to the public. You want the testing environment to scale inA.economical way. How should you design the process?", "choices": [{"id": 1996, "choice": " CreateascalableenvironmentinGCPforsimulatingproductionload\n", "answer": 1}, {"id": 1997, "choice": "UsetheexistinginfrastructuretotesttheGCP-basedbackendatscale\n", "answer": 0}, {"id": 1998, "choice": "Build stress tests into each component of your application using resources internal to GCP to simulate load\n", "answer": 0}, {"id": 1999, "choice": "Create a set of static environments in GCP to test different levels of load \u2013 for example, high, medium, and low", "answer": 0}]}, {"id": 119, "question": "Mountkirk Games wants to set up a continuous delivery pipeline. Their architecture includes many small services that they want to be able to update and roll back quickly. Mountkirk Games has the following requirements:\nServices are deployed redundantly across multiple regions in the US and Europe Only frontend services are exposed on the public internet\nThey can provide a single frontend IP for their fleet of services\nDeployment artifacts are immutable\nWhich set of products should they use?", "choices": [{"id": 2000, "choice": " GoogleCloudStorage,GoogleCloudDataflow,GoogleComputeEngine\n", "answer": 0}, {"id": 2001, "choice": "GoogleCloudStorage,GoogleAppEngine,GoogleNetworkLoadBalancer\n", "answer": 0}, {"id": 2002, "choice": "Google Kubernetes Registry, Google Container Engine, Google HTTP(S) Load Balancer ", "answer": 0}, {"id": 2003, "choice": "Google Cloud Functions, Google Cloud Pub/Sub, Google Cloud Deployment Manager", "answer": 1}]}, {"id": 120, "question": "Mountkirk Games\u2019 gaming servers are not automatically scaling properly. Last month, they rolled out a new feature, which suddenly became very popular. A record number of users are trying to use the service, but many of them are getting 503 errors and very slow response times. What should they investigate first?", "choices": [{"id": 2004, "choice": " Verifythatthedatabaseisonline\n", "answer": 0}, {"id": 2005, "choice": "Verifythattheprojectquotahasn\u2019tbeenexceeded\n", "answer": 1}, {"id": 2006, "choice": "Verify that the new feature code did not introduce any performance bugs ", "answer": 0}, {"id": 2007, "choice": "Verify that the load-testing team is not running their tool against production", "answer": 0}]}, {"id": 121, "question": "TerramEarth plans to connect all 20 million vehicles in the field to the cloud. This increases the volume to 20 million 600 byte records a second for 40 TBA.hour.\nHow should you design the data ingestion?", "choices": [{"id": 2008, "choice": " VehicleswritedatadirectlytoGCS\n", "answer": 0}, {"id": 2009, "choice": "VehicleswritedatadirectlytoGoogleCloudPub/Sub\n", "answer": 0}, {"id": 2010, "choice": "Vehicles stream data directly to Google BigQuery\n", "answer": 1}, {"id": 2011, "choice": "Vehicles continue to write data using the existing system (FTP)", "answer": 0}]}, {"id": 122, "question": "To speed up data retrieval, more vehicles will be upgraded to cellular connections and be able to transmit data to the ETL process. The current FTP process is error-prone and restarts the data transfer from the start of the file when connections fail, which happens often. You want to improve the reliability of the solution and minimize data transfer time on the cellular connections.    What should you do?", "choices": [{"id": 2012, "choice": " UseoneGoogleContainerEngineclusterofFTPservers.SavethedatatoaMulti-Regionalbucket.Run the ETL process using data in the bucket\n", "answer": 0}, {"id": 2013, "choice": "UsemultipleGoogleContainerEngineclustersrunningFTPserverslocatedindifferentregions.Savethe data to Multi-Regional buckets in US, EU, and Asia. Run the ETL process using the data in the bucket\n", "answer": 0}, {"id": 2014, "choice": "Directly transfer the files to different Google Cloud Multi-Regional Storage bucket locations in US, EU, and Asia using Google APIs over HTTP(S). Run the ETL process using the data in the bucket\n", "answer": 1}, {"id": 2015, "choice": "Directly transfer the files to a different Google Cloud Regional Storage bucket location in US, EU, and Asia using Google APIs over HTTP(S). Run the ETL process to retrieve the data from each Regional bucket", "answer": 0}]}, {"id": 123, "question": "TerramEarth\u2019s 20 million vehicles are scattered around the world. Based on the vehicle\u2019s location, its telemetry data is stored in a Google Cloud Storage (GCS) regional bucket (US, Europe, or Asia). The CTO has asked you to run a report on the raw telemetry data to determine why vehicles are breaking down after 100 K miles. You want to run this job on all the data.\nWhat is the most cost-effective way to run this job?", "choices": [{"id": 2016, "choice": " Moveallthedatainto1zone,thenlaunchaCloudDataprocclustertorunthejob\n", "answer": 0}, {"id": 2017, "choice": "Moveallthedatainto1region,thenlaunchaGoogleCloudDataprocclustertorunthejob\n", "answer": 0}, {"id": 2018, "choice": "Launch a cluster in each region to preprocess and compress the raw data, then move the data into a multi- region bucket and use a Dataproc cluster to finish the job\n", "answer": 1}, {"id": 2019, "choice": "Launch a cluster in each region to preprocess and compress the raw data, then move the data into a region bucket and use a Cloud Dataproc cluster to finish the job", "answer": 0}]}, {"id": 124, "question": "TerramEarth has equipped all connected trucks with servers and sensors to collect telemetry data. Next year they want to use the data to train machine learning models. They want to store this data in the cloud while reducing costs.\nWhat should they do?", "choices": [{"id": 2020, "choice": " Havethevehicle\u2019scomputercompressthedatainhourlysnapshots,andstoreitinaGoogleCloudStorage (GCS) Nearline bucket\n", "answer": 0}, {"id": 2021, "choice": "Pushthetelemetrydatainreal-timetoastreamingdataflowjobthatcompressesthedata,andstoreitin Google BigQuery\n", "answer": 0}, {"id": 2022, "choice": "Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Cloud Bigtable\n", "answer": 0}, {"id": 2023, "choice": "Have the vehicle\u2019s computer compress the data in hourly snapshots, and store it in a GCS Coldline bucket", "answer": 1}]}, {"id": 125, "question": "As part of Dress4Win's plans to migrate to the cloud, they want to be able to set up a managed logging and monitoring system so they can handle spikes in their traffic load.\nThey want to ensure that:\n* The infrastructure can be notified when it needs to scale up and down to handle the ebb and flow of usage throughout the day\n* Their administrators are notified automatically when their application reports errors.\n* They can filter their aggregated logs down in order to debug one piece of the application across many hosts\nWhich Google StackDriver features should they use?", "choices": [{"id": 2024, "choice": " Logging,Alerts,Insights,Debug\n", "answer": 0}, {"id": 2025, "choice": "Monitoring,Trace,Debug,Logging\n", "answer": 1}, {"id": 2026, "choice": "Monitoring, Logging, Alerts, Error Reporting ", "answer": 0}, {"id": 2027, "choice": "Monitoring, Logging, Debug, Error Report", "answer": 0}]}, {"id": 126, "question": "Dress4Win would like to become familiar with deploying applications to the cloud by successfully deploying some applications quickly,A.is. They have asked for your recommendation.\nWhat should you advise?", "choices": [{"id": 2028, "choice": " Identifyself-containedapplicationswithexternaldependenciesasafirstmovetothecloud.\n", "answer": 0}, {"id": 2029, "choice": "Identifyenterpriseapplicationswithinternaldependenciesandrecommendtheseasafirstmovetothe cloud.\n", "answer": 0}, {"id": 2030, "choice": "Suggest moving their in-house databases to the cloud and continue serving requests to on-premise applications.\n", "answer": 1}, {"id": 2031, "choice": "Recommend moving their message queuing servers to the cloud and continue handling requests to on- premise applications.", "answer": 0}]}, {"id": 127, "question": "Dress4Win has configured a new uptime check with Google Stackdriver for several of their legacy services. The Stackdriver dashboard is not reporting the servicesA.healthy.\nWhat should they do?", "choices": [{"id": 2032, "choice": " InstalltheStackdriveragentonallofthelegacywebservers.\n", "answer": 0}, {"id": 2033, "choice": "IntheCloudPlatformConsoledownloadthelistoftheuptimeservers'IPaddressesandcreateaninbound firewall rule\n", "answer": 0}, {"id": 2034, "choice": "Configure their load balancer to pass through the User-Agent HTTP header when the value matches GoogleStackdriverMonitoring-UptimeChecks (https://cloud.google.com/monitoring)\n", "answer": 0}, {"id": 2035, "choice": "Configure their legacy web servers to allow requests that contain user-Agent HTTP header when the value matches GoogleStackdriverMonitoring-UptimeChecks (https://cloud.google.com/monitoring)", "answer": 1}]}, {"id": 128, "question": "As part of their new application experience, Dress4Wm allows customers to upload images of themselves. The customer has exclusive control over who may view these images.\nCustomers should be able to upload images with minimal latency and also be shown their images quickly on the main application page when they log in.\nWhich configuration should Dress4Win use?", "choices": [{"id": 2036, "choice": " StoreimagefilesinaGoogleCloudStoragebucket.UseGoogleCloudDatastoretomaintainmetadata that maps each customer's ID and their image files.\n", "answer": 1}, {"id": 2037, "choice": "StoreimagefilesinaGoogleCloudStoragebucket.Addcustommetadatatotheuploadedimagesin Cloud Storage that contains the customer's unique ID.\n", "answer": 0}, {"id": 2038, "choice": "Use a distributed file system to store customers' images.A.storage needs increase, add more persistent disks and/or nodes. Assign each customer a unique ID, which sets each file's owner attribute, ensuring privacy of images.\n", "answer": 0}, {"id": 2039, "choice": "Use a distributed file system to store customers' images.A.storage needs increase, add more persistent disks and/or nodes. Use a Google Cloud SQL database to maintain metadata that maps each customer's ID to their image files.", "answer": 0}]}, {"id": 129, "question": "Dress4Win has end-to-end tests covering 100% of their endpoints.\nThey want to ensure that the move to the cloud does not introduce any new bugs.\n\n Which additional testing methods should the developers employ to preventA.outage?", "choices": [{"id": 2040, "choice": " TheyshouldenableGoogleStackdriverDebuggerontheapplicationcodetoshowerrorsinthecode.\n", "answer": 0}, {"id": 2041, "choice": "Theyshouldaddadditionalunittestsandproductionscaleloadtestsontheircloudstagingenvironment.\n", "answer": 1}, {"id": 2042, "choice": "They should run the end-to-end tests in the cloud staging environment to determine if the code is workingA.intended.\n", "answer": 0}, {"id": 2043, "choice": "They should add canary tests so developers can measure how much ofA.impact the new release causes to latency.", "answer": 0}]}, {"id": 130, "question": "For this question, refer to the Mountkirk Games case study. You need to analyze and define the technical architecture for the compute workloads for your company, Mountkirk Games. Considering the Mountkirk Games business and technical requirements, what should you do?", "choices": [{"id": 2044, "choice": " Createnetworkloadbalancers.UsepreemptibleComputeEngineinstances.\n", "answer": 0}, {"id": 2045, "choice": "Createnetworkloadbalancers.Usenon-preemptibleComputeEngineinstances.\n", "answer": 0}, {"id": 2046, "choice": "Create a global load balancer with managed instance groups and autoscaling policies. Use preemptible Compute Engine instances.             . ", "answer": 1}, {"id": 2047, "choice": "Create a global load balancer with managed instance groups and autoscaling policies. Use non- preemptible Compute Engine instances.", "answer": 0}]}, {"id": 131, "question": "For this question, refer to the Mountkirk Games case study. Mountkirk Games wants to design their solution for the future in order to take advantage of cloud and technology improvementsA.they become available. Which two steps should they take? (Choose two.)", "choices": [{"id": 2048, "choice": " Storeasmuchanalyticsandgameactivitydataasfinanciallyfeasibletodaysoitcanbeusedtotrain machine learning models to predict user behavior in the future.\n", "answer": 0}, {"id": 2049, "choice": "BeginpackagingtheirgamebackendartifactsincontainerimagesandrunningthemonKubernetesEngine to improve the availability to scale up or down based on game activity.\n", "answer": 0}, {"id": 2050, "choice": "Set up a CI/CD pipeline using Jenkins and Spinnaker to automate canary deployments and improve development velocity.\n", "answer": 1}, {"id": 2051, "choice": "Adopt a schema versioning tool to reduce downtime when adding new game features that require storing additional player data in the database.\n", "answer": 0}, {"id": 2052, "choice": "ImplementaweeklyrollingmaintenanceprocessfortheLinuxvirtualmachinessotheycanapplycritical kernel patches and package updates and reduce the risk of 0-day vulnerabilities.", "answer": 1}]}, {"id": 135, "question": "The operations manager asks you for a list of recommended practices that she should consider when migrating a J2EE application to the cloud.\nWhich three practices should you recommend? Choose 3 answers.", "choices": [{"id": 2065, "choice": " PorttheapplicationcodetorunonGoogleAppEngine\n", "answer": 1}, {"id": 2066, "choice": "IntegrateCloudDataflowintotheapplicationtocapturereal-timemetrics ", "answer": 0}, {"id": 2067, "choice": "Instrument the application with a monitoring tool like Stackdriver Debugge. \n ", "answer": 0}, {"id": 2068, "choice": "SelectA.automation framework to reliably provision the cloud infrastructure\n", "answer": 1}, {"id": 2069, "choice": "Deployacontinuousintegrationtoolwithautomatedtestinginastagingenvironment\n", "answer": 1}, {"id": 2070, "choice": "MigratefromMySQLtoamanagedNoSQLdatabaselikeGoogleCloudDatastoreorBigtable", "answer": 0}]}, {"id": 139, "question": "To reduce costs, the Director of Engineering has required all developers to move their development infrastructure resources from on-premises virtual machines (VMs) to Google Cloud Platform. These resources go through multiple start/stop events during the day and require state to persist. You have been asked to design the process of running a development environment in Google Cloud while providing cost visibility to the finance department.\nWhich two steps should you take? Choose 2 answers.", "choices": [{"id": 2084, "choice": " Usethe--no-auto-deleteflagonallpersistentdisksandstoptheVM\n", "answer": 0}, {"id": 2085, "choice": "Usethe--auto-deleteflagonallpersistentdisksandterminatetheVM\n", "answer": 0}, {"id": 2086, "choice": "ApplyVMCPUutilizationlabelandincludeitintheBigQuerybillingexport\n", "answer": 1}, {"id": 2087, "choice": "Use Google BigQuery billing export and labels to associate cost to groups\n", "answer": 0}, {"id": 2088, "choice": "StoreallstateintolocalSSD,snapshotthepersistentdisks,andterminatetheVM\n", "answer": 1}, {"id": 2089, "choice": "StoreallstateinGoogleCloudStorage,snapshotthepersistentdisks,andterminatetheVM", "answer": 0}]}, {"id": 145, "question": "Your customer is receiving reports that their recently updated Google App Engine application is taking approximately 30 seconds to load for some of their users. This behavior was not reported before the update.\nWhat strategy should you take?", "choices": [{"id": 2110, "choice": " WorkwithyourISPtodiagnosetheproblem\n", "answer": 0}, {"id": 2111, "choice": "Openasupporttickettoaskfornetworkcaptureandflowdatatodiagnosetheproblem,thenrollbackyour application\n", "answer": 0}, {"id": 2112, "choice": "Roll back toA.earlier known good release initially, then use Stackdriver Trace and Logging to diagnose the problem in a development/test/staging environment\n", "answer": 1}, {"id": 2113, "choice": "Roll back toA.earlier known good release, then push the release againA.a quieter period to investigate. Then use Stackdriver Trace and Logging to diagnose the problem", "answer": 0}]}, {"id": 147, "question": "Your application needs to process credit card transactions. You want the smallest scope of Payment Card Industry (PCI) compliance without compromising the ability to analyze transactional data and trends relating to which payment methods are used.\nHow should you design your architecture?", "choices": [{"id": 2119, "choice": " Createatokenizerserviceandstoreonlytokenizeddata\n", "answer": 1}, {"id": 2120, "choice": "Createseparateprojectsthatonlyprocesscreditcarddata\n", "answer": 0}, {"id": 2121, "choice": "Create separate subnetworks and isolate the components that process credit card data\n", "answer": 0}, {"id": 2122, "choice": "Streamline the audit discovery phase by labeling all of the virtual machines (VMs) that process PCI data\n", "answer": 0}, {"id": 2123, "choice": "EnableLoggingexporttoGoogleBigQueryanduseACLsandviewstoscopethedatasharedwiththe auditor", "answer": 0}]}, {"id": 151, "question": "The database administration team has asked you to help them improve the performance of their new database server running on Google Compute Engine. The database is for importing and normalizing their performance statistics and is built with MySQL running on Debian Linux. They haveA.n1-standard-8 virtual machine with 80 GB of SSD persistent disk.\nWhat should they change to get better performance from this system?", "choices": [{"id": 2136, "choice": " Increasethevirtualmachine\u2019smemoryto64GB\n", "answer": 0}, {"id": 2137, "choice": "CreateanewvirtualmachinerunningPostgreSQL\n", "answer": 0}, {"id": 2138, "choice": "Dynamically resize the SSD persistent disk to 500 GB\n", "answer": 1}, {"id": 2139, "choice": "Migrate their performance metrics warehouse to BigQuery\n", "answer": 0}, {"id": 2140, "choice": "Modifyalloftheirbatchjobstousebulkinsertsintothedatabase", "answer": 0}]}, {"id": 154, "question": "One of the developers on your team deployed their application in Google Container Engine with the Dockerfile below. They report that their application deployments are taking too long.        .  You want to optimize this Dockerfile for faster deployment times without adversely affecting the app\u2019s functionality.\nWhich two actions should you take? Choose 2 answers.", "choices": [{"id": 2149, "choice": " RemovePythonafterrunningpip\n", "answer": 0}, {"id": 2150, "choice": "Removedependenciesfromrequirements.txt\n", "answer": 0}, {"id": 2151, "choice": "Use a slimmed-down base image like Alpine Linux\n", "answer": 1}, {"id": 2152, "choice": "Use larger machine types for your Google Container Engine node pools\n", "answer": 0}, {"id": 2153, "choice": "Copythesourceafterhepackagedependencies(Pythonandpip)areinstalled", "answer": 1}]}, {"id": 156, "question": "A small number of API requests to your microservices-based application take a very long time. You know tha. . each request to the API can traverse many services. You want to know which service takes the longest in those cases.\nWhat should you do?", "choices": [{"id": 2158, "choice": " Settimeoutsonyourapplicationsothatyoucanfailrequestsfaster\n", "answer": 0}, {"id": 2159, "choice": "SendcustommetricsforeachofyourrequeststoStackdriverMonitoring\n", "answer": 0}, {"id": 2160, "choice": "Use Stackdriver Monitoring to look for insights that show when your API latencies are high\n", "answer": 0}, {"id": 2161, "choice": "Instrument your application with Stackdriver Trace in order to break down the request latenciesA.each microservice", "answer": 1}]}, {"id": 163, "question": "A development manager is building a new application. He asks you to review his requirements and identif. \n what cloud technologies he can use to meet them. The application must:\n1. Bebasedonopen-sourcetechnologyforcloudportability\n2. Dynamicallyscalecomputecapacitybasedondemand\n3. Supportcontinuoussoftwaredelivery\n4. Runmultiplesegregatedcopiesofthesameapplicationstack 5. Deployapplicationbundlesusingdynamictemplates\n6. RoutenetworktraffictospecificservicesbasedonURL\nWhich combination of technologies will meet all of his requirements?", "choices": [{"id": 2188, "choice": " GoogleKubernetesEngine,Jenkins,andHelm\n", "answer": 0}, {"id": 2189, "choice": "GoogleKubernetesEngineandCloudLoadBalancing\n", "answer": 0}, {"id": 2190, "choice": "Google Kubernetes Engine and Cloud Deployment Manager\n", "answer": 0}, {"id": 2191, "choice": "Google Kubernetes Engine, Jenkins, and Cloud Load Balancing", "answer": 1}]}, {"id": 167, "question": "As part of implementing their disaster recovery plan, your company is trying to replicate their production MySQL database from their private data center to their GCP project using a Google Cloud VPN connection. They are experiencing latency issues and a small amount of packet loss that is disrupting the replication. What should they do?", "choices": [{"id": 2203, "choice": " Configure their replication to use UDP .\n", "answer": 0}, {"id": 2204, "choice": "ConfigureaGoogleCloudDedicatedInterconnect.\n", "answer": 1}, {"id": 2205, "choice": "Restore their database daily using Google Cloud SQL.\n", "answer": 0}, {"id": 2206, "choice": "Add additional VPN connections and load balance them.\n", "answer": 0}, {"id": 2207, "choice": "SendthereplicatedtransactiontoGoogleCloudPub/Sub.", "answer": 0}]}, {"id": 168, "question": "You haveA.outage in your Compute Engine managed instance group: all instance keep restarting after 5 seconds. You have a health check configured, but autoscaling is disabled. Your colleague, who is a Linux expert, offered to look into the issue. You need to make sure that he can access the VMs. What should you do?", "choices": [{"id": 2208, "choice": " GrantyourcolleaguetheIAMroleofprojectViewer\n", "answer": 0}, {"id": 2209, "choice": "Performarollingrestartontheinstancegroup\n", "answer": 1}, {"id": 2210, "choice": "Disable the health check for the instance group. Add his SSH key to the project-wide SSH keys ", "answer": 0}, {"id": 2211, "choice": "Disable autoscaling for the instance group. Add his SSH key to the project-wide SSH Keys", "answer": 0}]}, {"id": 169, "question": "Your company is migrating its on-premises data center into the cloud.A.part of the migration, you want to integrate Kubernetes Engine for workload orchestration. Parts of your architecture must also be PCI DSS- compliant. Which of the following is most accurate?", "choices": [{"id": 2212, "choice": " AppEngineistheonlycomputeplatformonGCPthatiscertifiedforPCIDSShosting.\n", "answer": 0}, {"id": 2213, "choice": "KubernetesEnginecannotbeusedunderPCIDSSbecauseitisconsideredsharedhosting.\n", "answer": 0}, {"id": 2214, "choice": "Kubernetes Engine and GCP provide the tools you need to build a PCI DSS-compliant environment. ", "answer": 1}, {"id": 2215, "choice": "All Google Cloud services are usable because Google Cloud Platform is certified PCI-compliant.", "answer": 0}]}, {"id": 170, "question": "Your company has multiple on-premises systems that serveA.sources for reporting. The data has not been maintained well and has become degraded over time. You want to use Google-recommended practices to detect anomalies in your company data. What should you do?", "choices": [{"id": 2216, "choice": " UploadyourfilesintoCloudStorage.UseCloudDatalabtoexploreandcleanyourdata.\n", "answer": 1}, {"id": 2217, "choice": "UploadyourfilesintoCloudStorage.UseCloudDatapreptoexploreandcleanyourdata.\n", "answer": 0}, {"id": 2218, "choice": "Connect Cloud Datalab to your on-premises systems. Use Cloud Datalab to explore and clean your data.\n", "answer": 0}, {"id": 2219, "choice": "Connect Cloud Dataprep to your on-premises systems. Use Cloud Dataprep to explore and clean your data.", "answer": 0}]}, {"id": 171, "question": "Google Cloud Platform resources are managed hierarchically using organization, folders, and projects. When Cloud Identity and Access Management (IAM) policies existA.these different levels, what is the effective policyA.a particular node of the hierarchy?", "choices": [{"id": 2220, "choice": " Theeffectivepolicyisdeterminedonlybythepolicysetatthenode\n", "answer": 0}, {"id": 2221, "choice": "Theeffectivepolicyisthepolicysetatthenodeandrestrictedbythepoliciesofitsancestors\n", "answer": 0}, {"id": 2222, "choice": "The effective policy is the union of the policy setA.the node and policies inherited from its ancestors\n", "answer": 1}, {"id": 2223, "choice": "The effective policy is the intersection of the policy setA.the node and policies inherited from its ancestors", "answer": 0}]}, {"id": 172, "question": "You are migrating your on-premises solution to Google Cloud in several phases. You will use Cloud VPN to maintain a connection between your on-premises systems and Google Cloud until the migration is completed. You want to make sure all your on-premises systems remain reachable during this period. How should you organize your networking in Google Cloud?", "choices": [{"id": 2224, "choice": " UsethesameIPrangeonGoogleCloudasyouuseon-premises\n", "answer": 0}, {"id": 2225, "choice": "UsethesameIPrangeonGoogleCloudasyouuseon-premisesforyourprimaryIPrangeandusea secondary range that does not overlap with the range you use on-premises\n", "answer": 0}, {"id": 2226, "choice": "UseA.IP range on Google Cloud that does not overlap with the range you use on-premises\n", "answer": 0}, {"id": 2227, "choice": "UseA.IP range on Google Cloud that does not overlap with the range you use on-premises for your primary IP range and use a secondary range with the same IP rangeA.you use on-premises", "answer": 1}]}, {"id": 173, "question": "You have foundA.error in your App Engine application caused by missing Cloud Datastore indexes. You have created a YAML file with the required indexes and want to deploy these new indexes to Cloud Datastore. What should you do?", "choices": [{"id": 2228, "choice": " Pointgclouddatastorecreate-indexestoyourconfigurationfile\n", "answer": 0}, {"id": 2229, "choice": "UploadtheconfigurationfiletheAppEngine\u2019sdefaultCloudStoragebucket,andhaveAppEnginedetect\nthe new indexes\n", "answer": 0}, {"id": 2230, "choice": "In the GCP Console, use Datastore Admin to delete the current indexes and upload the new configuration file\n", "answer": 1}, {"id": 2231, "choice": "CreateA.HTTP request to the built-in python module to send the index configuration file to your application", "answer": 0}]}, {"id": 174, "question": "You haveA.application that will run on Compute Engine. You need to designA.architecture that takes into account a disaster recovery plan that requires your application to fail over to another region in case of a regional outage. What should you do?", "choices": [{"id": 2232, "choice": " DeploytheapplicationontwoComputeEngineinstancesinthesameprojectbutinadifferentregion.Use the first instance to serve traffic, and use the HTTP load balancing service to fail over to the standby instance in case of a disaster.\n", "answer": 0}, {"id": 2233, "choice": "DeploytheapplicationonaComputeEngineinstance.Usetheinstancetoservetraffic,andusetheHTTP load balancing service to fail over toA.instance on your premises in case of a disaster.\n", "answer": 1}, {"id": 2234, "choice": "Deploy the application on two Compute Engine instance groups, each in the same project but in a different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster.\n", "answer": 0}, {"id": 2235, "choice": "Deploy the application on two Compute Engine instance groups, each in separate project and a different region. Use the first instance group to server traffic, and use the HTTP load balancing service to fail over to the standby instance in case of a disaster.", "answer": 0}]}, {"id": 175, "question": "You are deployingA.application on App Engine that needs to integrate withA.on-premises database. For security purposes, your on-premises database must not be accessible through the public Internet. What should you do?", "choices": [{"id": 2236, "choice": " DeployyourapplicationonAppEnginestandardenvironmentanduseAppEnginefirewallrulestolimit access to the open on-premises database.\n", "answer": 1}, {"id": 2237, "choice": "DeployyourapplicationonAppEnginestandardenvironmentanduseCloudVPNtolimitaccesstotheon- premises database.\n", "answer": 0}, {"id": 2238, "choice": "Deploy your application on App Engine flexible environment and use App Engine firewall rules to limit access to the on-premises database.\n", "answer": 0}, {"id": 2239, "choice": "Deploy your application on App Engine flexible environment and use Cloud VPN to limit access to the on- premises database.", "answer": 0}]}, {"id": 176, "question": "You are working in a highly secured environment where public Internet access from the Compute Engine VMs is not allowed. You do not yet have a VPN connection to accessA.on-premises file server. You need to install specific software on a Compute Engine instance. How should you install the software?", "choices": [{"id": 2240, "choice": " UploadtherequiredinstallationfilestoCloudStorage.ConfiguretheVMonasubnetwithaPrivateGoogle Access subnet. Assign onlyA.internal IP address to the V", "answer": 0}, {"id": 2241, "choice": "Download the installation files to the VM using gsutil.\n", "answer": 1}, {"id": 2242, "choice": "UploadtherequiredinstallationfilestoCloudStorageandusefirewallrulestoblockalltrafficexcepttheIP address range for Cloud Storage. Download the files to the VM using gsutil.\n", "answer": 0}, {"id": 2243, "choice": "Upload the required installation files to Cloud Source Repositories. Configure the VM on a subnet with a Private Google Access subnet. Assign onlyA.internal IP address to the V", "answer": 0}, {"id": 2244, "choice": "Download the installation files to the VM using gcloud.\n", "answer": 0}, {"id": 2245, "choice": "Upload the required installation files to Cloud Source Repositories and use firewall rules to block all traffic except the IP address range for Cloud Source Repositories. Download the files to the VM using gsutil.", "answer": 0}]}, {"id": 177, "question": "Your company is moving 75 TB of data into Google Cloud. You want to use Cloud Storage and follow Google- recommended practices. What should you do?", "choices": [{"id": 2246, "choice": " MoveyourdataontoaTransferAppliance.UseaTransferApplianceRehydratortodecryptthedatainto Cloud Storage.\n", "answer": 0}, {"id": 2247, "choice": "MoveyourdataontoaTransferAppliance.UseCloudDatapreptodecryptthedataintoCloudStorage.\n", "answer": 0}, {"id": 2248, "choice": "Install gsutil on each server that contains data. Use resumable transfers to upload the data into Cloud Storage.\n", "answer": 1}, {"id": 2249, "choice": "Install gsutil on each server containing data. Use streaming transfers to upload the data into Cloud Storage.", "answer": 0}]}, {"id": 178, "question": "You haveA.application deployed on Kubernetes Engine using a Deployment named echo-deployment. The deployment is exposed using a Service called echo-service. You need to performA.update to the application with minimal downtime to the application. What should you do?", "choices": [{"id": 2250, "choice": " Usekubectlsetimagedeployment/echo-deployment<new-image>\n", "answer": 0}, {"id": 2251, "choice": "UsetherollingupdatefunctionalityoftheInstanceGroupbehindtheKubernetescluster\n", "answer": 1}, {"id": 2252, "choice": "Update the deployment yaml file with the new container image. Use kubectl delete deployment/ echo-deployment and kubectl create \u2013f <yaml-file>\n", "answer": 0}, {"id": 2253, "choice": "Update the service yaml file which the new container image. Use kubectl delete service/echo- service and kubectl create \u2013f <yaml-file>", "answer": 0}]}, {"id": 179, "question": "Your company is using BigQueryA.its enterprise data warehouse. Data is distributed over several Google Cloud projects. All queries on BigQuery need to be billed on a single project. You want to make sure that no query costs are incurred on the projects that contain the data. Users should be able to query the datasets, but not edit them.\nHow should you configure users\u2019 access roles?", "choices": [{"id": 2254, "choice": " Addalluserstoagroup.GrantthegrouptheroleofBigQueryuseronthebillingprojectandBigQuery dataViewer on the projects that contain the data.\n", "answer": 1}, {"id": 2255, "choice": "Addalluserstoagroup.GrantthegrouptherolesofBigQuerydataVieweronthebillingprojectand BigQuery user on the projects that contain the data.\n", "answer": 0}, {"id": 2256, "choice": "Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.\n", "answer": 0}, {"id": 2257, "choice": "Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that contain the data.", "answer": 0}]}, {"id": 180, "question": "You have developedA.application using Cloud ML Engine that recognizes famous paintings from uploade.  . images. You want to test the application and allow specific people to upload images for the next 24 hours. Not all users have a Google Account. How should you have users upload images?", "choices": [{"id": 2258, "choice": " HaveusersuploadtheimagestoCloudStorage.Protectthebucketwithapasswordthatexpiresafter24 hours.\n", "answer": 1}, {"id": 2259, "choice": "HaveusersuploadtheimagestoCloudStorageusingasignedURLthatexpiresafter24hours.\n", "answer": 0}, {"id": 2260, "choice": "CreateA.App Engine web application where users can upload images. Configure App Engine to disable the application after 24 hours. Authenticate users via Cloud Identity.\n", "answer": 0}, {"id": 2261, "choice": "CreateA.App Engine web application where users can upload images for the next 24 hours. Authenticate users via Cloud Identity.", "answer": 0}]}, {"id": 181, "question": "Your web application must comply with the requirements of the European Union\u2019s General Data Protection Regulation (GDPR). You are responsible for the technical architecture of your web application. What should you do?", "choices": [{"id": 2262, "choice": " EnsurethatyourwebapplicationonlyusesnativefeaturesandservicesofGoogleCloudPlatform,because Google already has various certifications and provides \u201cpass-on\u201d compliance when you use native features.\n", "answer": 0}, {"id": 2263, "choice": "EnabletherelevantGDPRcompliancesettingwithintheGCPConsoleforeachoftheservicesinusewithin your application.\n", "answer": 0}, {"id": 2264, "choice": "Ensure that Cloud Security Scanner is part of your test planning strategy in order to pick up any compliance gaps.\n", "answer": 0}, {"id": 2265, "choice": "Define a design for the security of data in your web application that meets GDPR requirements.", "answer": 1}]}], "GCP Professional Data Engineer": [{"id": 182, "question": "Your company is migrating their 30-node Apache Hadoop cluster to the cloud. They want to re-use Hadoop jobs they have already created and minimize the management of the cluster as much as possible. They also want to be able to persist data beyond the life of the cluster. What should you do?", "choices": [{"id": 2266, "choice": " Create a Google Cloud Dataflow job to process the data.\n", "answer": 1}, {"id": 2267, "choice": "Create a Google Cloud Dataproc cluster that uses persistent disks for HDFS.\n", "answer": 0}, {"id": 2268, "choice": "Create a Hadoop cluster on Google Compute Engine that uses persistent disks.\n", "answer": 0}, {"id": 2269, "choice": "Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector.\n", "answer": 0}, {"id": 2270, "choice": "Create a Hadoop cluster on Google Compute Engine that uses Local SSD disks.", "answer": 0}]}, {"id": 183, "question": "Business owners at your company have given you a database of bank transactions. Each row contains the user ID, transaction type, transaction location, and transaction amount. They ask you to investigate what type of machine learning can be applied to the data. Which three machine learning applications can you use? (Choose three.)", "choices": [{"id": 2271, "choice": " Supervised learning to determine which transactions are most likely to be fraudulent.\n", "answer": 0}, {"id": 2272, "choice": "Unsupervised learning to determine which transactions are most likely to be fraudulent.\n", "answer": 1}, {"id": 2273, "choice": "Clustering to divide the transactions into N categories based on feature similarity.\n", "answer": 1}, {"id": 2274, "choice": "Supervised learning to predict the location of a transaction.\n", "answer": 0}, {"id": 2275, "choice": "Reinforcement learning to predict the location of a transaction.\n", "answer": 1}, {"id": 2276, "choice": "Unsupervised learning to predict the location of a transaction.", "answer": 0}]}, {"id": 184, "question": "Your company\u2019s on-premises Apache Hadoop servers are approaching end-of-life, and IT has decided to migrate the cluster to Google Cloud Dataproc. A like-for-like migration of the cluster would require 50 TB of Google Persistent Disk per node. The CIO is concerned about the cost of using that much block storage. You want to minimize the storage cost of the migration. What should you do?", "choices": [{"id": 2277, "choice": " Put the data into Google Cloud Storage.\n", "answer": 0}, {"id": 2278, "choice": "Use preemptible virtual machines (VMs) for the Cloud Dataproc cluster.\n", "answer": 1}, {"id": 2279, "choice": "Tune the Cloud Dataproc cluster so that there is just enough disk for all data.\n", "answer": 0}, {"id": 2280, "choice": "Migrate some of the cold data into Google Cloud Storage, and keep only the hot data in Persistent Disk.", "answer": 0}]}, {"id": 185, "question": "You work for a car manufacturer and have set up a data pipeline using Google Cloud Pub/Sub to capture anomalous sensor events. You are using a push subscription in Cloud Pub/Sub that calls a custom HTTPS endpoint that you have created to take action of these anomalous events as they occur. Your custom HTTPS endpoint keeps getting an inordinate amount of duplicate messages. What is the most likely cause of these duplicate messages?", "choices": [{"id": 2281, "choice": " The message body for the sensor event is too large.\n", "answer": 0}, {"id": 2282, "choice": "Your custom endpoint has an out-of-date SSL certificate.\n", "answer": 1}, {"id": 2283, "choice": "The Cloud Pub/Sub topic has too many messages published to it.\n", "answer": 0}, {"id": 2284, "choice": "Your custom endpoint is not acknowledging messages within the acknowledgement deadline.", "answer": 0}]}, {"id": 186, "question": "Your company uses a proprietary system to send inventory data every 6 hours to a data ingestion service in the cloud. Transmitted data includes a payload of several fields and the timestamp of the transmission. If there are any concerns about a transmission, the system re-transmits the data. How should you deduplicate the data most efficiency?", "choices": [{"id": 2285, "choice": " Assign global unique identifiers (GUID) to each data entry.\n", "answer": 0}, {"id": 2286, "choice": "Compute the hash value of each data entry, and compare it with all historical data.\n", "answer": 0}, {"id": 2287, "choice": "Store each data entry as the primary key in a separate database and apply an index.\n", "answer": 0}, {"id": 2288, "choice": "Maintain a database table to store the hash value and other metadata for each data entry.", "answer": 1}]}, {"id": 187, "question": "Your company has hired a new data scientist who wants to perform complicated analyses across very large datasets stored in Google Cloud Storage and in a Cassandra cluster on Google Compute Engine. The scientist primarily wants to create labelled data sets for machine learning projects, along with some visualization tasks. She reports that her laptop is not powerful enough to perform her tasks and it is slowing her down. You want to help her perform her tasks. What should you do?", "choices": [{"id": 2289, "choice": " Run a local version of Jupiter on the laptop.\n", "answer": 0}, {"id": 2290, "choice": "Grant the user access to Google Cloud Shell.\n", "answer": 1}, {"id": 2291, "choice": "Host a visualization tool on a VM on Google Compute Engine.\n", "answer": 0}, {"id": 2292, "choice": "Deploy Google Cloud Datalab to a virtual machine (VM) on Google Compute Engine.", "answer": 0}]}, {"id": 188, "question": "You are deploying 10,000 new Internet of Things devices to collect temperature data in your warehouses globally. You need to process, store and analyze these very large datasets in real time. What should you do?", "choices": [{"id": 2293, "choice": " Send the data to Google Cloud Datastore and then export to BigQuery.\n", "answer": 0}, {"id": 2294, "choice": "Send the data to Google Cloud Pub/Sub, stream Cloud Pub/Sub to Google Cloud Dataflow, and store the data in Google BigQuery.\n", "answer": 1}, {"id": 2295, "choice": "Send the data to Cloud Storage and then spin up an Apache Hadoop cluster as needed in Google Cloud Dataproc whenever analysis is required.\n", "answer": 0}, {"id": 2296, "choice": "Export logs in batch to Google Cloud Storage and then spin up a Google Cloud SQL instance, import the data from Cloud Storage, and run an analysis as needed.", "answer": 0}]}, {"id": 189, "question": "You have spent a few days loading data from comma-separated values (CSV) files into the Google BigQuery table CLICK_STREAM. The column DT stores the epoch time of click events. For convenience, you chose a simple schema where every field is treated as the STRING type. Now, you want to compute web session durations of users who visit your site, and you want to change its data type to the TIMESTAMP. You want to minimize the migration effort without making future queries computationally expensive. What should you do?", "choices": [{"id": 2297, "choice": " Delete the table CLICK_STREAM, and then re-create it such that the column DT is of the TIMESTAMP type. Reload the data.\n", "answer": 0}, {"id": 2298, "choice": "Add a column TS of the TIMESTAMP type to the table CLICK_STREAM, and populate the numeric values from the column TS for each row. Reference the column TS instead of the column DT from now on.\n", "answer": 0}, {"id": 2299, "choice": "Create a view CLICK_STREAM_V, where strings from the column DT are cast into TIMESTAMP values. Reference the view CLICK_STREAM_V instead of the table CLICK_STREAM from now on.\n", "answer": 0}, {"id": 2300, "choice": "Add two columns to the table CLICK STREAM: TS of the TIMESTAMP type and IS_NEW of the BOOLEAN type. Reload all data in append mode. For each appended row, set the value of IS_NEW to true. For future queries, reference the column TS instead of the column DT, with the WHERE clause ensuring that the value of IS_NEW must be true.\n", "answer": 1}, {"id": 2301, "choice": "Construct a query to return every row of the table CLICK_STREAM, while using the built-in function to cast strings from the column DT into TIMESTAMP values. Run the query into a destination table NEW_CLICK_STREAM, in which the column TS is the TIMESTAMP type. Reference the table NEW_CLICK_STREAM instead of the table CLICK_STREAM from now on. In the future, new data is loaded into the table NEW_CLICK_STREAM.", "answer": 0}]}, {"id": 190, "question": "You want to use Google Stackdriver Logging to monitor Google BigQuery usage. You need an instant notification to be sent to your monitoring tool when new data is appended to a certain table using an insert job, but you do not want to receive notifications for other tables. What should you do?", "choices": [{"id": 2302, "choice": " Make a call to the Stackdriver API to list all logs, and apply an advanced filter.\n", "answer": 0}, {"id": 2303, "choice": "In the Stackdriver logging admin interface, and enable a log sink export to BigQuery.\n", "answer": 1}, {"id": 2304, "choice": "In the Stackdriver logging admin interface, enable a log sink export to Google Cloud Pub/Sub, and subscribe to the topic from your monitoring tool.\n", "answer": 0}, {"id": 2305, "choice": "Using the Stackdriver API, create a project sink with advanced log filter to export to Pub/Sub, and subscribe to the topic from your monitoring tool.", "answer": 0}]}, {"id": 191, "question": "You are working on a sensitive project involving private user data. You have set up a project on Google Cloud Platform to house your work internally. An external consultant is going to assist with coding a complex transformation in a Google Cloud Dataflow pipeline for your project. How should you maintain users\u2019 privacy?", "choices": [{"id": 2306, "choice": " Grant the consultant the Viewer role on the project.\n", "answer": 0}, {"id": 2307, "choice": "Grant the consultant the Cloud Dataflow Developer role on the project.\n", "answer": 0}, {"id": 2308, "choice": "Create a service account and allow the consultant to log on with it.\n", "answer": 1}, {"id": 2309, "choice": "Create an anonymized sample of the data for the consultant to work with in a different project.", "answer": 0}]}, {"id": 192, "question": "You are building a model to predict whether or not it will rain on a given day. You have thousands of input features and want to see if you can improve training speed by removing some features while having a minimum effect on model accuracy. What can you do?", "choices": [{"id": 2310, "choice": " Eliminate features that are highly correlated to the output labels.\n", "answer": 0}, {"id": 2311, "choice": "Combine highly co-dependent features into one representative feature.\n", "answer": 1}, {"id": 2312, "choice": "Instead of feeding in each feature individually, average their values in batches of 3.\n", "answer": 0}, {"id": 2313, "choice": "Remove the features that have null values for more than 50% of the training records.", "answer": 0}]}, {"id": 193, "question": "Your company is running their first dynamic campaign, serving different offers by analyzing real-time data during the holiday season. The data scientists are collecting terabytes of data that rapidly grows every hour during their 30-day campaign. They are using Google Cloud Dataflow to preprocess the data and collect the feature (signals) data that is needed for the machine learning model in Google Cloud Bigtable. The team is observing suboptimal performance with reads and writes of their initial load of 10 TB of data. They want to improve this performance while minimizing cost. What should they do?", "choices": [{"id": 2314, "choice": " Redefine the schema by evenly distributing reads and writes across the row space of the table.\n", "answer": 1}, {"id": 2315, "choice": "The performance issue should be resolved over time as the site of the BigDate cluster is increased.\n", "answer": 0}, {"id": 2316, "choice": "Redesign the schema to use a single row key to identify values that need to be updated frequently in the cluster.\n", "answer": 0}, {"id": 2317, "choice": "Redesign the schema to use row keys based on numeric IDs that increase sequentially per user viewing the offers.", "answer": 0}]}, {"id": 194, "question": "Your software uses a simple JSON format for all messages. These messages are published to Google Cloud Pub/Sub, then processed with Google Cloud Dataflow to create a real-time dashboard for the CFO. During testing, you notice that some messages are missing in the dashboard. You check the logs, and all messages are being published to Cloud Pub/Sub successfully. What should you do next?", "choices": [{"id": 2318, "choice": " Check the dashboard application to see if it is not displaying correctly.\n", "answer": 0}, {"id": 2319, "choice": "Run a fixed dataset through the Cloud Dataflow pipeline and analyze the output.\n", "answer": 1}, {"id": 2320, "choice": "Use Google Stackdriver Monitoring on Cloud Pub/Sub to find the missing messages.\n", "answer": 0}, {"id": 2321, "choice": "Switch Cloud Dataflow to pull messages from Cloud Pub/Sub instead of Cloud Pub/Sub pushing messages to Cloud Dataflow.", "answer": 0}]}, {"id": 195, "question": "Company Overview\nFlowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.\nCompany Background\nThe company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.\nSolution Concept\nFlowlogistic wants to implement two concepts using the cloud:\nUse their proprietary technology in a real-time inventory-tracking system that indicates the location of their loadsPerform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.\nExisting Technical Environment\nFlowlogistic architecture resides in a single data center:\nDatabases\n8 physical servers in 2 clusters\n- SQL Server \u2013 user data, inventory, static data 3 physical servers\n- Cassandra \u2013 metadata, tracking messages\n10 Kafka servers \u2013 tracking message aggregation and batch insert\nApplication servers \u2013 customer front end, middleware for order/customs 60 virtual machines across 20 physical servers\n- Tomcat \u2013 Java services - Nginx \u2013 static content\n- Batch servers\nStorage appliances\n- iSCSI for virtual machine (VM) hosts\n- Fibre Channel storage area network (FC SAN) \u2013 SQL server storage - Network-attached storage (NAS) image storage, logs, backups\nApache Hadoop /Spark servers - Core Data Lake\n- Data analysis workloads\n20 miscellaneous servers\n- Jenkins, monitoring, bastion hosts,\nBusiness Requirements\nBuild a reliable and reproducible environment with scaled panty of production.\nAggregate data in a centralized Data Lake for analysis\nUse historical data to perform predictive analytics on future shipments\nAccurately track every shipment worldwide using proprietary technology\nImprove business agility and speed of innovation through rapid provisioning of new resources Analyze and optimize architecture for performance in the cloud\nMigrate fully to the cloud if all other requirements are met\nTechnical Requirements\nHandle both streaming and batch data\nMigrate existing Hadoop workloads\nEnsure architecture is scalable and elastic to meet the changing demands of the company. Use managed services whenever possible\nEncrypt data flight and at rest\nConnect a VPN between the production data center and cloud environment\nSEO Statement\nWe have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around. We need to organize our information so we can more easily understand where our customers are and what they are shipping.\nCTO Statement\nIT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to                  implement the CFO\u2019 s tracking technology.\nCFO Statement\nPart of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don\u2019t want to commit capital to building out a server environment.\nFlowlogistic wants to use Google BigQuery as their primary analysis system, but they still have Apache Hadoop and Spark workloads that they cannot move to BigQuery. Flowlogistic does not know how to store the data that is common to both workloads. What should they do?", "choices": [{"id": 2322, "choice": " Store the common data in BigQuery as partitioned tables.\n", "answer": 0}, {"id": 2323, "choice": "Store the common data in BigQuery and expose authorized views.\n", "answer": 1}, {"id": 2324, "choice": "Store the common data encoded as Avro in Google Cloud Storage.\n", "answer": 0}, {"id": 2325, "choice": "Store he common data in the HDFS storage for a Google Cloud Dataproc cluster.", "answer": 0}]}, {"id": 196, "question": "Company Overview\nFlowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.\nCompany Background\nThe company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.\nSolution Concept\nFlowlogistic wants to implement two concepts using the cloud:\nUse their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads\nPerform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.\nExisting Technical Environment\nFlowlogistic architecture resides in a single data center:\nDatabases\n8 physical servers in 2 clusters\n- SQL Server \u2013 user data, inventory, static data 3 physical servers  - Cassandra \u2013 metadata, tracking messages\n10 Kafka servers \u2013 tracking message aggregation and batch insert\nApplication servers \u2013 customer front end, middleware for order/customs 60 virtual machines across 20 physical servers\n- Tomcat \u2013 Java services - Nginx \u2013 static content\n- Batch servers\nStorage appliances\n- iSCSI for virtual machine (VM) hosts\n- Fibre Channel storage area network (FC SAN) \u2013 SQL server storage - Network-attached storage (NAS) image storage, logs, backups\nApache Hadoop /Spark servers - Core Data Lake\n- Data analysis workloads\n20 miscellaneous servers\n- Jenkins, monitoring, bastion hosts,\nBusiness Requirements\nBuild a reliable and reproducible environment with scaled panty of production.\nAggregate data in a centralized Data Lake for analysis\nUse historical data to perform predictive analytics on future shipments\nAccurately track every shipment worldwide using proprietary technology\nImprove business agility and speed of innovation through rapid provisioning of new resources Analyze and optimize architecture for performance in the cloud\nMigrate fully to the cloud if all other requirements are met\nTechnical Requirements\nHandle both streaming and batch data\nMigrate existing Hadoop workloads\nEnsure architecture is scalable and elastic to meet the changing demands of the company. Use managed services whenever possible\nEncrypt data flight and at rest\nConnect a VPN between the production data center and cloud environment\nSEO Statement\nWe have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around. We need to organize our information so we can more easily understand where our customers are and what they are shipping.\nCTO Statement\nIT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO\u2019 s tracking technology.\nCFO Statement\nPart of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don\u2019t want to commit capital to building out a server environment.\nFlowlogistic\u2019s management has determined that the current Apache Kafka servers cannot handle the data volume for their real-time inventory tracking system. You need to build a new system on Google Cloud Platform (GCP) that will feed the proprietary tracking software. The system must be able to ingest data from a variety of global sources, process and query in real-time, and store the data reliably. Which combination of GCP                products should you choose?", "choices": [{"id": 2326, "choice": " Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage\n", "answer": 0}, {"id": 2327, "choice": "Cloud Pub/Sub, Cloud Dataflow, and Local SSD\n", "answer": 0}, {"id": 2328, "choice": "Cloud Pub/Sub, Cloud SQL, and Cloud Storage\n", "answer": 1}, {"id": 2329, "choice": "Cloud Load Balancing, Cloud Dataflow, and Cloud Storage", "answer": 0}]}, {"id": 197, "question": "Company Overview\nFlowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.\nCompany Background\nThe company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.\nSolution Concept\nFlowlogistic wants to implement two concepts using the cloud:\nUse their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads\nPerform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.\nExisting Technical Environment\nFlowlogistic architecture resides in a single data center:\nDatabases\n8 physical servers in 2 clusters\n- SQL Server \u2013 user data, inventory, static data 3 physical servers\n- Cassandra \u2013 metadata, tracking messages\n10 Kafka servers \u2013 tracking message aggregation and batch insert\nApplication servers \u2013 customer front end, middleware for order/customs 60 virtual machines across 20 physical servers\n- Tomcat \u2013 Java services - Nginx \u2013 static content\n- Batch servers\nStorage appliances\n- iSCSI for virtual machine (VM) hosts    - Fibre Channel storage area network (FC SAN) \u2013 SQL server storage\n- Network-attached storage (NAS) image storage, logs, backups Apache Hadoop /Spark servers\n- Core Data Lake\n- Data analysis workloads 20 miscellaneous servers\n- Jenkins, monitoring, bastion hosts,\nBusiness Requirements\nBuild a reliable and reproducible environment with scaled panty of production.\nAggregate data in a centralized Data Lake for analysis\nUse historical data to perform predictive analytics on future shipments\nAccurately track every shipment worldwide using proprietary technology\nImprove business agility and speed of innovation through rapid provisioning of new resources Analyze and optimize architecture for performance in the cloud\nMigrate fully to the cloud if all other requirements are met\nTechnical Requirements\nHandle both streaming and batch data\nMigrate existing Hadoop workloads\nEnsure architecture is scalable and elastic to meet the changing demands of the company. Use managed services whenever possible\nEncrypt data flight and at rest\nConnect a VPN between the production data center and cloud environment\nSEO Statement\nWe have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around. We need to organize our information so we can more easily understand where our customers are and what they are shipping.\nCTO Statement\nIT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO\u2019 s tracking technology.\nCFO Statement\nPart of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don\u2019t want to commit capital to building out a server environment.\nFlowlogistic\u2019s CEO wants to gain rapid insight into their customer base so his sales team can be better informed in the field. This team is not very technical, so they\u2019ve purchased a visualization tool to simplify the creation of BigQuery reports. However, they\u2019ve been overwhelmed by all the data in the table, and are spending a lot of money on queries trying to find the data they need. You want to solve their problem in the most cost-effective way. What should you do?", "choices": [{"id": 2330, "choice": " Export the data into a Google Sheet for virtualization.\n", "answer": 0}, {"id": 2331, "choice": "Create an additional table with only the necessary columns.\n", "answer": 0}, {"id": 2332, "choice": "Create a view on the table to present to the virtualization tool.\n", "answer": 1}, {"id": 2333, "choice": "Create identity and access management (IAM) roles on the appropriate columns, so only they appear in a query. ", "answer": 0}]}, {"id": 198, "question": "Company Overview\nFlowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.\nCompany Background\nThe company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.\nSolution Concept\nFlowlogistic wants to implement two concepts using the cloud:\nUse their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads\nPerform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.\nExisting Technical Environment\nFlowlogistic architecture resides in a single data center:\nDatabases\n8 physical servers in 2 clusters\n- SQL Server \u2013 user data, inventory, static data 3 physical servers\n- Cassandra \u2013 metadata, tracking messages\n10 Kafka servers \u2013 tracking message aggregation and batch insert\nApplication servers \u2013 customer front end, middleware for order/customs 60 virtual machines across 20 physical servers\n- Tomcat \u2013 Java services - Nginx \u2013 static content\n- Batch servers\nStorage appliances\n- iSCSI for virtual machine (VM) hosts\n- Fibre Channel storage area network (FC SAN) \u2013 SQL server storage - Network-attached storage (NAS) image storage, logs, backups\nApache Hadoop /Spark servers - Core Data Lake\n- Data analysis workloads\n20 miscellaneous servers\n- Jenkins, monitoring, bastion hosts,      Business Requirements\nBuild a reliable and reproducible environment with scaled panty of production.\nAggregate data in a centralized Data Lake for analysis\nUse historical data to perform predictive analytics on future shipments\nAccurately track every shipment worldwide using proprietary technology\nImprove business agility and speed of innovation through rapid provisioning of new resources Analyze and optimize architecture for performance in the cloud\nMigrate fully to the cloud if all other requirements are met\nTechnical Requirements\nHandle both streaming and batch data\nMigrate existing Hadoop workloads\nEnsure architecture is scalable and elastic to meet the changing demands of the company. Use managed services whenever possible\nEncrypt data flight and at rest\nConnect a VPN between the production data center and cloud environment\nSEO Statement\nWe have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around. We need to organize our information so we can more easily understand where our customers are and what they are shipping.\nCTO Statement\nIT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO\u2019 s tracking technology.\nCFO Statement\nPart of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don\u2019t want to commit capital to building out a server environment.\nFlowlogistic is rolling out their real-time inventory tracking system. The tracking devices will all send package- tracking messages, which will now go to a single Google Cloud Pub/Sub topic instead of the Apache Kafka cluster. A subscriber application will then process the messages for real-time reporting and store them in Google BigQuery for historical analysis. You want to ensure the package data can be analyzed over time. Which approach should you take?", "choices": [{"id": 2334, "choice": " Attach the timestamp on each message in the Cloud Pub/Sub subscriber application as they are received.\n", "answer": 0}, {"id": 2335, "choice": "Attach the timestamp and Package ID on the outbound message from each publisher device as they are sent to Clod Pub/Sub.\n", "answer": 1}, {"id": 2336, "choice": "Use the NOW () function in BigQuery to record the event\u2019s time.\n", "answer": 0}, {"id": 2337, "choice": "Use the automatically generated timestamp from Cloud Pub/Sub to order the data.", "answer": 0}]}, {"id": 199, "question": "MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\nCompany Background\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\nSolution Concept\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs: Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.\nRefine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.\nMJTelco will also use three separate operating environments \u2013 development/test, staging, and production \u2013 to meet the needs of running experiments, deploying new features, and serving production customers.\nBusiness Requirements\nScale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.\nEnsure security of their proprietary data to protect their leading-edge machine learning and analysis. Provide reliable and timely access to data for analysis from distributed research workers\nMaintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\nTechnical Requirements\nEnsure secure and efficient transport and storage of telemetry data\nRapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each. Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\nSupport rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\nCEO Statement\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.\nCTO Statement\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.         CFO Statement\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2019s machine learning will allow our quantitative researchers to work on our high- value problems instead of problems with our data pipelines.\nMJTelco\u2019s Google Cloud Dataflow pipeline is now ready to start receiving data from the 50,000 installations. You want to allow Cloud Dataflow to scale its compute power up as required. Which Cloud Dataflow pipeline configuration setting should you update?", "choices": [{"id": 2338, "choice": " The zone\n", "answer": 1}, {"id": 2339, "choice": "The number of workers\n", "answer": 0}, {"id": 2340, "choice": "The disk size per worker\n", "answer": 0}, {"id": 2341, "choice": "The maximum number of workers", "answer": 0}]}, {"id": 200, "question": "Company Overview\nMJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\nCompany Background\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\nSolution Concept\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs: Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.\nRefine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.\nMJTelco will also use three separate operating environments \u2013 development/test, staging, and production \u2013 to meet the needs of running experiments, deploying new features, and serving production customers.\nBusiness Requirements Scale up their production environment with minimal cost, instantiating resources when and where needed inA. unpredictable, distributed telecom user community.\nEnsure security of their proprietary data to protect their leading-edge machine learning and analysis. Provide reliable and timely access to data for analysis from distributed research workers\nMaintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\nTechnical Requirements\nEnsure secure and efficient transport and storage of telemetry data\nRapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each. Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\nSupport rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\nCEO Statement\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.\nCTO Statement\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.\nCFO Statement\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2019s machine learning will allow our quantitative researchers to work on our high- value problems instead of problems with our data pipelines.\nYou need to compose visualizations for operations teams with the following requirements:\nThe report must include telemetry data from all 50,000 installations for the most resent 6 weeks (sampling once every minute).\nThe report must not be more than 3 hours delayed from live data.\nThe actionable report should only show suboptimal links.\nMost suboptimal links should be sorted to the top.\nSuboptimal links can be grouped and filtered by regional geography.\nUser response time to load the report must be <5 seconds.\nWhich approach meets the requirements?", "choices": [{"id": 2342, "choice": " Load the data into Google Sheets, use formulas to calculate a metric, and use filters/sorting to show only suboptimal links in a table.\n", "answer": 0}, {"id": 2343, "choice": "Load the data into Google BigQuery tables, write Google Apps Script that queries the data, calculates the metric, and shows only suboptimal rows in a table in Google Sheets.\n", "answer": 0}, {"id": 2344, "choice": "Load the data into Google Cloud Datastore tables, write a Google App Engine Application that queries all rows, applies a function to derive the metric, and then renders results in a table using the Google charts and visualization API.\n", "answer": 1}, {"id": 2345, "choice": "Load the data into Google BigQuery tables, write a Google Data Studio 360 report that connects to your data, calculates a metric, and then uses a filter expression to show only suboptimal rows in a table.", "answer": 0}]}, {"id": 201, "question": "Company Overview\nMJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\nCompany Background\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\nSolution Concept\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs: Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.\nRefine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.\nMJTelco will also use three separate operating environments \u2013 development/test, staging, and production \u2013 to meet the needs of running experiments, deploying new features, and serving production customers.\nBusiness Requirements\nScale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.\nEnsure security of their proprietary data to protect their leading-edge machine learning and analysis. Provide reliable and timely access to data for analysis from distributed research workers\nMaintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\nTechnical Requirements\nEnsure secure and efficient transport and storage of telemetry data\nRapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each. Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\nSupport rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\nCEO Statement\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.         CTO Statement\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.\nCFO Statement\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2019s machine learning will allow our quantitative researchers to work on our high- value problems instead of problems with our data pipelines.\nYou create a new report for your large team in Google Data Studio 360. The report uses Google BigQuery as its data source. It is company policy to ensure employees can view only the data associated with their region, so you create and populate a table for each region. You need to enforce the regional access policy to the data. Which two actions should you take? (Choose two.)", "choices": [{"id": 2346, "choice": " Ensure all the tables are included in global dataset.\n", "answer": 0}, {"id": 2347, "choice": "Ensure each table is included in a dataset for a region.\n", "answer": 1}, {"id": 2348, "choice": "Adjust the settings for each table to allow a related region-based security group view access.\n", "answer": 0}, {"id": 2349, "choice": "Adjust the settings for each view to allow a related region-based security group view access.\n", "answer": 1}, {"id": 2350, "choice": "Adjust the settings for each dataset to allow a related region-based security group view access.", "answer": 0}]}, {"id": 202, "question": "Company Overview\nMJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\nCompany Background\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\nSolution Concept\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs: Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology\ndefinition.\nMJTelco will also use three separate operating environments \u2013 development/test, staging, and production \u2013 to meet the needs of running experiments, deploying new features, and serving production customers.\nBusiness Requirements\nScale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.\nEnsure security of their proprietary data to protect their leading-edge machine learning and analysis. Provide reliable and timely access to data for analysis from distributed research workers\nMaintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\nTechnical Requirements\nEnsure secure and efficient transport and storage of telemetry data\nRapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each. Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\nSupport rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\nCEO Statement\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.\nCTO Statement\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.\nCFO Statement\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud\u2019s machine learning will allow our quantitative researchers to work on our high- value problems instead of problems with our data pipelines.\nMJTelco needs you to create a schema in Google Bigtable that will allow for the historical analysis of the last 2 years of records. Each record that comes in is sent every 15 minutes, and contains a unique identifier of the device and a data record. The most common query is for all the data for a given device for a given day. Which schema should you use?", "choices": [{"id": 2351, "choice": " Rowkey: date#device_id Column data: data_point\n", "answer": 0}, {"id": 2352, "choice": "Rowkey: date\nColumn data: device_id, data_point\n", "answer": 0}, {"id": 2353, "choice": "Rowkey: device_id\nColumn data: date, data_point\n", "answer": 0}, {"id": 2354, "choice": "Rowkey: data_point\nColumn data: device_id, date", "answer": 1}]}, {"id": 203, "question": "Your company has recently grown rapidly and now ingesting data at a significantly higher rate than it was previously. You manage the daily batch MapReduce analytics jobs in Apache Hadoop. However, the recent increase in data has meant the batch jobs are falling behind. You were asked to recommend ways the development team could increase the responsiveness of the analytics without increasing costs. What should you recommend they do?", "choices": [{"id": 2355, "choice": " Rewrite the job in Pig.\n", "answer": 1}, {"id": 2356, "choice": "Rewrite the job in Apache Spark.\n", "answer": 0}, {"id": 2357, "choice": "Increase the size of the Hadoop cluster.\n", "answer": 0}, {"id": 2358, "choice": "Decrease the size of the Hadoop cluster but also rewrite the job in Hive.", "answer": 0}]}, {"id": 204, "question": "You work for a large fast food restaurant chain with over 400,000 employees. You store employee information in Google BigQuery in a Users table consisting of a FirstName field and a LastName field. A member of IT is building an application and asks you to modify the schema and data in BigQuery so the application can query a FullName field consisting of the value of the FirstName field concatenated with a space, followed by the value of the LastName field for each employee. How can you make that data available while minimizing cost?", "choices": [{"id": 2359, "choice": " Create a view in BigQuery that concatenates the FirstName and LastName field values to produce the FullName.\n", "answer": 0}, {"id": 2360, "choice": "Add a new column called FullName to the Users table. Run an UPDATE statement that updates the FullName column for each user with the concatenation of the FirstName and LastName values.\n", "answer": 0}, {"id": 2361, "choice": "Create a Google Cloud Dataflow job that queries BigQuery for the entire Users table, concatenates the FirstName value and LastName value for each user, and loads the proper values for FirstName, LastName, and FullName into a new table in BigQuery.\n", "answer": 1}, {"id": 2362, "choice": "Use BigQuery to export the data for the table to a CSV file. Create a Google Cloud Dataproc job to process the CSV file and output a new CSV file containing the proper values for FirstName, LastName and FullName. Run a BigQuery load job to load the new CSV file into BigQuery.", "answer": 0}]}, {"id": 205, "question": "You are deploying a new storage system for your mobile application, which is a media streaming service. You decide the best fit is Google Cloud Datastore. You have entities with multiple properties, some of which can take on multiple values. For example, in the entity \u2018Movie\u2019 the property \u2018actors\u2019 and the property \u2018tags\u2019\nhavemultiplevaluesbuttheproperty\u2018date released\u2019 doesnot.Atypicalquerywouldaskforallmovies with actor=<actorname> ordered by date_released or all movies with tag=Comedy ordered by date_released. Howshouldyouavoidacombinatorialexplosioninthenumberofindexes?\nC. Set the following in your entity options: exclude_from_indexes = \u2018actors, tags\u2019\nD. Set the following in your entity options: exclude_from_indexes = \u2018date_published\u2019", "choices": [{"id": 2363, "choice": " Option A\n", "answer": 1}, {"id": 2364, "choice": "Option B.\n", "answer": 0}, {"id": 2365, "choice": "Option C\n", "answer": 0}, {"id": 2366, "choice": "Option D", "answer": 0}]}, {"id": 206, "question": "You work for a manufacturing plant that batches application log files together into a single log file once a day at 2:00 AM. You have written a Google Cloud Dataflow job to process that log file. You need to make sure the log file in processed once per day as inexpensively as possible. What should you do?", "choices": [{"id": 2367, "choice": " Change the processing job to use Google Cloud Dataproc instead.\n", "answer": 0}, {"id": 2368, "choice": "Manually start the Cloud Dataflow job each morning when you get into the office.\n", "answer": 0}, {"id": 2369, "choice": "Create a cron job with Google App Engine Cron Service to run the Cloud Dataflow job. ", "answer": 1}, {"id": 2370, "choice": "Configure the Cloud Dataflow job as a streaming job so that it processes the log data immediately.", "answer": 0}]}, {"id": 207, "question": "You work for an economic consulting firm that helps companies identify economic trends as they happen. As part of your analysis, you use Google BigQuery to correlate customer data with the average prices of the 100 most common goods sold, including bread, gasoline, milk, and others. The average prices of these goods are updated every 30 minutes. You want to make sure this data stays up to date so you can combine it with other data in BigQuery as cheaply as possible. What should you do?", "choices": [{"id": 2371, "choice": " Load the data every 30 minutes into a new partitioned table in BigQuery.\n", "answer": 1}, {"id": 2372, "choice": "Store and update the data in a regional Google Cloud Storage bucket and create a federated data source in BigQuery\n", "answer": 0}, {"id": 2373, "choice": "Store the data in Google Cloud Datastore. Use Google Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Cloud Datastore\n", "answer": 0}, {"id": 2374, "choice": "Store the data in a file in a regional Google Cloud Storage bucket. Use Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Google Cloud Storage.", "answer": 0}]}, {"id": 208, "question": "You are designing the database schema for a machine learning-based food ordering service that will predict what users want to eat. Here is some of the information you need to store:\nThe user profile: What the user likes and doesn\u2019t like to eat\nThe user account information: Name, address, preferred meal times The order information: When orders are made, from where, to whom\nThe database will be used to store all the transactional data of the product. You want to optimize the data schema. Which Google Cloud Platform product should you use?", "choices": [{"id": 2375, "choice": " BigQuery\n", "answer": 1}, {"id": 2376, "choice": "Cloud SQL\n", "answer": 0}, {"id": 2377, "choice": "Cloud Bigtable\n", "answer": 0}, {"id": 2378, "choice": "Cloud Datastore", "answer": 0}]}, {"id": 209, "question": "Your company is loading comma-separated values (CSV) files into Google BigQuery. The data is fully imported successfully; however, the imported data is not matching byte-to-byte to the source file. What is the most likely cause of this problem?", "choices": [{"id": 2379, "choice": " The CSV data loaded in BigQuery is not flagged as CSV.\n", "answer": 0}, {"id": 2380, "choice": "The CSV data has invalid rows that were skipped on import.\n", "answer": 1}, {"id": 2381, "choice": "The CSV data loaded in BigQuery is not using BigQuery\u2019s default encoding.\n", "answer": 0}, {"id": 2382, "choice": "The CSV data has not gone through an ETL phase before loading into BigQuery.", "answer": 0}]}, {"id": 210, "question": "Your company produces 20,000 files every hour. Each data file is formatted as a comma separated values (CSV) file that is less than 4 KB. All files must be ingested on Google Cloud Platform before they can be processed. Your company site has a 200 ms latency to Google Cloud, and your Internet connection bandwidth is limited as 50 Mbps. You currently deploy a secure FTP (SFTP) server on a virtual machine in Google Compute Engine as the data ingestion point. A local SFTP client runs on a dedicated machine to transmit the CSV files as is. The goal is to make reports with data from the previous day available to the executives by 10:00 a.m. each day. This design is barely able to keep up with the current volume, even though the bandwidth utilization is rather low.\nYou are told that due to seasonality, your company expects the number of files to double for the next three months. Which two actions should you take? (choose two.)", "choices": [{"id": 2383, "choice": " Introduce data compression for each file to increase the rate file of file transfer.\n", "answer": 0}, {"id": 2384, "choice": "Contact your internet service provider (ISP) to increase your maximum bandwidth to at least 100 Mbps.\n", "answer": 0}, {"id": 2385, "choice": "Redesign the data ingestion process to use gsutil tool to send the CSV files to a storage bucket in parallel.\n", "answer": 1}, {"id": 2386, "choice": "Assemble 1,000 files into a tape archive (TAR) file. Transmit the TAR files instead, and disassemble the CSV files in the cloud upon receiving them.\n", "answer": 0}, {"id": 2387, "choice": "Create an S3-compatible storage endpoint in your network, and use Google Cloud Storage Transfer Service to transfer on-premices data to the designated storage bucket.", "answer": 1}]}, {"id": 211, "question": "You are choosing a NoSQL database to handle telemetry data submitted from millions of Internet-of-Things (IoT) devices. The volume of data is growing at 100 TB per year, and each data entry has about 100 attributes. The data processing pipeline does not require atomicity, consistency, isolation, and durability (ACID). However, high availability and low latency are required.\nYou need to analyze the data by querying against individual fields. Which three databases meet your requirements? (Choose three.)", "choices": [{"id": 2388, "choice": " Redis\n", "answer": 0}, {"id": 2389, "choice": "HBase\n", "answer": 1}, {"id": 2390, "choice": "MySQL\n", "answer": 0}, {"id": 2391, "choice": "MongoDB\n", "answer": 1}, {"id": 2392, "choice": "Cassandra\n", "answer": 0}, {"id": 2393, "choice": "HDFS with Hive", "answer": 1}]}, {"id": 212, "question": "Suppose you have a table that includes a nested column called \"city\" inside a column called \"person\", but when you try to submit the following query in BigQuery, it gives you an error.\nSELECT person FROM `project1.example.table1` WHERE city = \"London\"\nHow would you correct the error?", "choices": [{"id": 2394, "choice": " Add \", UNNEST(person)\" before the WHERE clause.\n", "answer": 1}, {"id": 2395, "choice": "Change \"person\" to \"person.city\".\n", "answer": 0}, {"id": 2396, "choice": "Change \"person\" to \"city.person\".\n", "answer": 0}, {"id": 2397, "choice": "Add \", UNNEST(city)\" before the WHERE clause.", "answer": 0}]}, {"id": 213, "question": "What are two of the benefits of using denormalized data structures in BigQuery?", "choices": [{"id": 2398, "choice": " Reduces the amount of data processed, reduces the amount of storage required\n", "answer": 0}, {"id": 2399, "choice": "Increases query speed, makes queries simpler\n", "answer": 1}, {"id": 2400, "choice": "Reduces the amount of storage required, increases query speed\n", "answer": 0}, {"id": 2401, "choice": "Reduces the amount of data processed, increases query speed", "answer": 0}]}, {"id": 214, "question": "Which of these statements about exporting data from BigQuery is false?", "choices": [{"id": 2402, "choice": " To export more than 1 GB of data, you need to put a wildcard in the destination filename.\n", "answer": 0}, {"id": 2403, "choice": "The only supported export destination is Google Cloud Storage.\n", "answer": 0}, {"id": 2404, "choice": "Data can only be exported in JSON or Avro format.\n", "answer": 1}, {"id": 2405, "choice": "The only compression option available is GZIP.", "answer": 0}]}, {"id": 215, "question": "What are all of the BigQuery operations that Google charges for?", "choices": [{"id": 2406, "choice": " Storage, queries, and streaming inserts\n", "answer": 1}, {"id": 2407, "choice": "Storage, queries, and loading data from a file\n", "answer": 0}, {"id": 2408, "choice": "Storage, queries, and exporting data\n", "answer": 0}, {"id": 2409, "choice": "Queries and streaming inserts", "answer": 0}]}, {"id": 216, "question": "Which of the following is not possible using primitive roles?", "choices": [{"id": 2410, "choice": " Give a user viewer access to BigQuery and owner access to Google Compute Engine instances.\n", "answer": 0}, {"id": 2411, "choice": "Give UserA owner access and UserB editor access for all datasets in a project.\n", "answer": 0}, {"id": 2412, "choice": "Give a user access to view all datasets in a project, but not run queries on them.\n", "answer": 1}, {"id": 2413, "choice": "Give GroupA owner access and GroupB editor access for all datasets in a project.", "answer": 0}]}, {"id": 217, "question": "Which of these statements about BigQuery caching is true?", "choices": [{"id": 2414, "choice": " By default, a query's results are not cached.\n", "answer": 0}, {"id": 2415, "choice": "BigQuery caches query results for 48 hours.\n", "answer": 0}, {"id": 2416, "choice": "Query results are cached even if you specify a destination table.\n", "answer": 0}, {"id": 2417, "choice": "There is no charge for a query that retrieves its results from cache.", "answer": 1}]}, {"id": 218, "question": "Which of these sources can you not load data into BigQuery from?", "choices": [{"id": 2418, "choice": " File upload\n", "answer": 0}, {"id": 2419, "choice": "Google Drive\n", "answer": 0}, {"id": 2420, "choice": "Google Cloud Storage\n", "answer": 0}, {"id": 2421, "choice": "Google Cloud SQL", "answer": 1}]}, {"id": 219, "question": "Which of the following statements about Legacy SQL and Standard SQL is not true?", "choices": [{"id": 2422, "choice": " Standard SQL is the preferred query language for BigQuery.\n", "answer": 0}, {"id": 2423, "choice": "If you write a query in Legacy SQL, it might generate an error if you try to run it with Standard SQL.\n", "answer": 0}, {"id": 2424, "choice": "One difference between the two query languages is how you specify fully-qualified table names (i.e. table names that include their associated project name).\n", "answer": 0}, {"id": 2425, "choice": "You need to set a query language for each dataset and the default is Standard SQL.", "answer": 1}]}, {"id": 220, "question": "How would you query specific partitions in a BigQuery table?", "choices": [{"id": 2426, "choice": " Use the DAY column in the WHERE clause\n", "answer": 0}, {"id": 2427, "choice": "Use the EXTRACT(DAY) clause\n", "answer": 0}, {"id": 2428, "choice": "Use the __PARTITIONTIME pseudo-column in the WHERE clause\n", "answer": 1}, {"id": 2429, "choice": "Use DATE BETWEEN in the WHERE clause", "answer": 0}]}, {"id": 221, "question": "Which SQL keyword can be used to reduce the number of columns processed by BigQuery?", "choices": [{"id": 2430, "choice": " BETWEEN ", "answer": 0}, {"id": 2431, "choice": "WHERE ", "answer": 0}, {"id": 2432, "choice": "SELECT ", "answer": 1}, {"id": 2433, "choice": "LIMIT", "answer": 0}]}, {"id": 222, "question": "To give a user read permission for only the first three columns of a table, which access control method would you use?", "choices": [{"id": 2434, "choice": " Primitive role\n", "answer": 0}, {"id": 2435, "choice": "Predefined role\n", "answer": 0}, {"id": 2436, "choice": "Authorized view\n", "answer": 1}, {"id": 2437, "choice": "It's not possible to give access to only the first three columns of a table.", "answer": 0}]}, {"id": 223, "question": "What are two methods that can be used to denormalize tables in BigQuery?", "choices": [{"id": 2438, "choice": " 1) Split table into multiple tables; 2) Use a partitioned table\n", "answer": 0}, {"id": 2439, "choice": "1) Join tables into one table; 2) Use nested repeated fields\n", "answer": 1}, {"id": 2440, "choice": "1) Use a partitioned table; 2) Join tables into one table\n", "answer": 0}, {"id": 2441, "choice": "1) Use nested repeated fields; 2) Use a partitioned table", "answer": 0}]}, {"id": 224, "question": "Which of these is not a supported method of putting data into a partitioned table?", "choices": [{"id": 2442, "choice": " If you have existing data in a separate file for each day, then create a partitioned table and upload each file into the appropriate partition.\n", "answer": 0}, {"id": 2443, "choice": "Run a query to get the records for a specific day from an existing table and for the destination table, specify a partitioned table ending with the day in the format \"$YYYYMMDD\".\n", "answer": 0}, {"id": 2444, "choice": "Create a partitioned table and stream new records to it every day.\n", "answer": 0}, {"id": 2445, "choice": "Use ORDER BY to put a table's rows into chronological order and then change the table's type to \"Partitioned\".", "answer": 1}]}, {"id": 225, "question": "Which of these operations can you perform from the BigQuery Web UI?", "choices": [{"id": 2446, "choice": " Upload a file in SQL format.\n", "answer": 0}, {"id": 2447, "choice": "Load data with nested and repeated fields.\n", "answer": 1}, {"id": 2448, "choice": "Upload a 20 MB file.\n", "answer": 0}, {"id": 2449, "choice": "Upload multiple files using a wildcard.", "answer": 0}]}, {"id": 226, "question": "Which methods can be used to reduce the number of rows processed by BigQuery?", "choices": [{"id": 2450, "choice": " Splitting tables into multiple tables; putting data in partitions\n", "answer": 1}, {"id": 2451, "choice": "Splitting tables into multiple tables; putting data in partitions; using the LIMIT clause\n", "answer": 0}, {"id": 2452, "choice": "Putting data in partitions; using the LIMIT clause\n", "answer": 0}, {"id": 2453, "choice": "Splitting tables into multiple tables; using the LIMIT clause", "answer": 0}]}, {"id": 227, "question": "Why do you need to split a machine learning dataset into training data and test data?", "choices": [{"id": 2454, "choice": " So you can try two different sets of features\n", "answer": 0}, {"id": 2455, "choice": "To make sure your model is generalized for more than just the training data\n", "answer": 1}, {"id": 2456, "choice": "To allow you to create unit tests in your code\n", "answer": 0}, {"id": 2457, "choice": "So you can use one dataset for a wide model and one for a deep model", "answer": 0}]}, {"id": 228, "question": "Which of these numbers are adjusted by a neural network as it learns from a training dataset (select 2 answers)?", "choices": [{"id": 2458, "choice": " W eights\n", "answer": 1}, {"id": 2459, "choice": "Biases\n", "answer": 1}, {"id": 2460, "choice": "Continuous features\n", "answer": 0}, {"id": 2461, "choice": "Input values", "answer": 0}]}, {"id": 229, "question": "The CUSTOM tier for Cloud Machine Learning Engine allows you to specify the number of which types of cluster nodes?", "choices": [{"id": 2462, "choice": " Workers\n", "answer": 0}, {"id": 2463, "choice": "Masters, workers, and parameter servers\n", "answer": 0}, {"id": 2464, "choice": "Workers and parameter servers\n", "answer": 1}, {"id": 2465, "choice": "Parameter servers", "answer": 0}]}, {"id": 230, "question": "Which software libraries are supported by Cloud Machine Learning Engine?", "choices": [{"id": 2466, "choice": " Theano and TensorFlow\n", "answer": 0}, {"id": 2467, "choice": "Theano and Torch\n", "answer": 0}, {"id": 2468, "choice": "TensorFlow\n", "answer": 1}, {"id": 2469, "choice": "TensorFlow and Torch", "answer": 0}]}, {"id": 231, "question": "Which TensorFlow function can you use to configure a categorical column if you don't know all of the possible values for that column?", "choices": [{"id": 2470, "choice": " categorical_column_with_vocabulary_list ", "answer": 0}, {"id": 2471, "choice": "categorical_column_with_hash_bucket\n", "answer": 1}, {"id": 2472, "choice": "categorical_column_with_unknown_values ", "answer": 0}, {"id": 2473, "choice": "sparse_column_with_keys", "answer": 0}]}, {"id": 232, "question": "Which of the following statements about the Wide & Deep Learning model are true? (Select 2 answers.)", "choices": [{"id": 2474, "choice": " The wide model is used for memorization, while the deep model is used for generalization.\n", "answer": 1}, {"id": 2475, "choice": "A good use for the wide and deep model is a recommender system.\n", "answer": 1}, {"id": 2476, "choice": "The wide model is used for generalization, while the deep model is used for memorization.\n", "answer": 0}, {"id": 2477, "choice": "A good use for the wide and deep model is a small-scale linear regression problem.", "answer": 0}]}, {"id": 233, "question": "To run a TensorFlow training job on your own computer using Cloud Machine Learning Engine, what would your command start with?", "choices": [{"id": 2478, "choice": " gcloud ml-engine local train\n", "answer": 1}, {"id": 2479, "choice": "gcloud ml-engine jobs submit training\n", "answer": 0}, {"id": 2480, "choice": "gcloud ml-engine jobs submit training local\n", "answer": 0}, {"id": 2481, "choice": "You can't run a TensorFlow program on your own computer using Cloud ML Engine .", "answer": 0}]}, {"id": 234, "question": "If you want to create a machine learning model that predicts the price of a particular stock based on its recent price history, what type of estimator should you use?", "choices": [{"id": 2482, "choice": " Unsupervised learning\n", "answer": 0}, {"id": 2483, "choice": "Regressor\n", "answer": 1}, {"id": 2484, "choice": "Classifier\n", "answer": 0}, {"id": 2485, "choice": "Clustering estimator", "answer": 0}]}, {"id": 235, "question": "Suppose you have a dataset of images that are each labeled as to whether or not they contain a human face. To create a neural network that recognizes human faces in images using this labeled dataset, what approach would likely be the most effective?", "choices": [{"id": 2486, "choice": " Use K-means Clustering to detect faces in the pixels.\n", "answer": 0}, {"id": 2487, "choice": "Use feature engineering to add features for eyes, noses, and mouths to the input data.\n", "answer": 0}, {"id": 2488, "choice": "Use deep learning by creating a neural network with multiple hidden layers to automatically detect features of faces.\n", "answer": 1}, {"id": 2489, "choice": "Build a neural network with an input layer of pixels, a hidden layer, and an output layer with two categories.", "answer": 0}]}, {"id": 236, "question": "What are two of the characteristics of using online prediction rather than batch prediction?", "choices": [{"id": 2490, "choice": " It is optimized to handle a high volume of data instances in a job and to run more complex models.\n", "answer": 0}, {"id": 2491, "choice": "Predictions are returned in the response message.\n", "answer": 1}, {"id": 2492, "choice": "Predictions are written to output files in a Cloud Storage location that you specify.\n", "answer": 0}, {"id": 2493, "choice": "It is optimized to minimize the latency of serving predictions.", "answer": 1}]}, {"id": 237, "question": "Which of these are examples of a value in a sparse vector? (Select 2 answers.)", "choices": [{"id": 2494, "choice": " [0,5,0,0,0,0] ", "answer": 0}, {"id": 2495, "choice": "[0,0,0,1,0,0,1] ", "answer": 0}, {"id": 2496, "choice": "[0, 1]\n", "answer": 1}, {"id": 2497, "choice": "[1,0,0,0,0,0,0]", "answer": 1}]}, {"id": 238, "question": "How can you get a neural network to learn about relationships between categories in a categorical feature?", "choices": [{"id": 2498, "choice": " Create a multi-hot column\n", "answer": 0}, {"id": 2499, "choice": "Create a one-hot column\n", "answer": 0}, {"id": 2500, "choice": "Create a hash bucket\n", "answer": 0}, {"id": 2501, "choice": "Create an embedding column", "answer": 1}]}, {"id": 239, "question": "If a dataset contains rows with individual people and columns for year of birth, country, and income, how many of the columns are continuous and how many are categorical?", "choices": [{"id": 2502, "choice": " 1 continuous and 2 categorical\n", "answer": 0}, {"id": 2503, "choice": "3 categorical\n", "answer": 0}, {"id": 2504, "choice": "3 continuous\n", "answer": 0}, {"id": 2505, "choice": "2 continuous and 1 categorical", "answer": 1}]}, {"id": 240, "question": "Which of the following are examples of hyperparameters? (Select 2 answers.)", "choices": [{"id": 2506, "choice": " Number of hidden layers\n", "answer": 1}, {"id": 2507, "choice": "Number of nodes in each hidden layer\n", "answer": 1}, {"id": 2508, "choice": "Biases\n", "answer": 0}, {"id": 2509, "choice": "Weights", "answer": 0}]}, {"id": 241, "question": "Which of the following are feature engineering techniques? (Select 2 answers)", "choices": [{"id": 2510, "choice": " Hidden feature layers\n", "answer": 0}, {"id": 2511, "choice": "Feature prioritization\n", "answer": 0}, {"id": 2512, "choice": "Crossed feature columns\n", "answer": 1}, {"id": 2513, "choice": "Bucketization of a continuous feature", "answer": 1}]}, {"id": 242, "question": "You want to use a BigQuery table as a data sink. In which writing mode(s) can you use BigQuery as a sink?", "choices": [{"id": 2514, "choice": " Both batch and streaming\n", "answer": 1}, {"id": 2515, "choice": "BigQuery cannot be used as a sink\n", "answer": 0}, {"id": 2516, "choice": "Onlybatch\n", "answer": 0}, {"id": 2517, "choice": "Onlystreaming", "answer": 0}]}, {"id": 243, "question": "You have a job that you want to cancel. It is a streaming pipeline, and you want to ensure that any data that is in-flight is processed and written to the output. Which of the following commands can you use on the Dataflow monitoring console to stop the pipeline job?", "choices": [{"id": 2518, "choice": " Cancel ", "answer": 0}, {"id": 2519, "choice": "Drain ", "answer": 1}, {"id": 2520, "choice": "Stop ", "answer": 0}, {"id": 2521, "choice": "Finish", "answer": 0}]}, {"id": 244, "question": "When running a pipeline that has a BigQuery source, on your local machine, you continue to get permission\ndenied errors. What could be the reason for that?", "choices": [{"id": 2522, "choice": " Your gcloud does not have access to the BigQuery resources\n", "answer": 1}, {"id": 2523, "choice": "BigQuery cannot be accessed from local machines\n", "answer": 0}, {"id": 2524, "choice": "You are missing gcloud on your machine\n", "answer": 0}, {"id": 2525, "choice": "Pipelines cannot be run locally", "answer": 0}]}, {"id": 245, "question": "What Dataflow concept determines when a Window's contents should be output based on certain criteria being met?", "choices": [{"id": 2526, "choice": " Sessions\n", "answer": 0}, {"id": 2527, "choice": "OutputCriteria ", "answer": 0}, {"id": 2528, "choice": "Windows\n", "answer": 0}, {"id": 2529, "choice": "Triggers", "answer": 1}]}, {"id": 246, "question": "Which of the following is NOT one of the three main types of triggers that Dataflow supports?", "choices": [{"id": 2530, "choice": " Trigger based on element size in bytes\n", "answer": 1}, {"id": 2531, "choice": "Trigger that is a combination of other triggers\n", "answer": 0}, {"id": 2532, "choice": "Trigger based on element count\n", "answer": 0}, {"id": 2533, "choice": "Trigger based on time", "answer": 0}]}, {"id": 247, "question": "Which Java SDK class can you use to run your Dataflow programs locally?", "choices": [{"id": 2534, "choice": " LocalRunner\n", "answer": 0}, {"id": 2535, "choice": "DirectPipelineRunner ", "answer": 1}, {"id": 2536, "choice": "MachineRunner\n", "answer": 0}, {"id": 2537, "choice": "LocalPipelineRunner", "answer": 0}]}, {"id": 248, "question": "The Dataflow SDKs have been recently transitioned into which Apache service?", "choices": [{"id": 2538, "choice": " Apache Spark\n", "answer": 0}, {"id": 2539, "choice": "Apache Hadoop\n", "answer": 0}, {"id": 2540, "choice": "Apache Kafka\n", "answer": 0}, {"id": 2541, "choice": "Apache Beam", "answer": 1}]}, {"id": 249, "question": "The ____________ for Cloud Bigtable makes it possible to use Cloud Bigtable in a Cloud Dataflow pipeline.", "choices": [{"id": 2542, "choice": " Cloud Dataflow connector\n", "answer": 1}, {"id": 2543, "choice": "DataFlow SDK\n", "answer": 0}, {"id": 2544, "choice": "BiqQueryAPI\n", "answer": 0}, {"id": 2545, "choice": "BigQuery Data Transfer Service", "answer": 0}]}, {"id": 250, "question": "Does Dataflow process batch data pipelines or streaming data pipelines?", "choices": [{"id": 2546, "choice": " Only Batch Data Pipelines\n", "answer": 0}, {"id": 2547, "choice": "Both Batch and Streaming Data Pipelines\n", "answer": 1}, {"id": 2548, "choice": "OnlyStreamingDataPipelines\n", "answer": 0}, {"id": 2549, "choice": "None of the above", "answer": 0}]}, {"id": 251, "question": "You are planning to use Google's Dataflow SDK to analyze customer data such as displayed below. Your project requirement is to extract only the customer name from the data source and then write to an output PCollection.\nTom,555 X street\nTim,553 Y street\nSam, 111 Z street\nWhich operation is best suited for the above data processing requirement?", "choices": [{"id": 2550, "choice": " ParDo\n", "answer": 1}, {"id": 2551, "choice": "Sink API\n", "answer": 0}, {"id": 2552, "choice": "Source API\n", "answer": 0}, {"id": 2553, "choice": "Data extraction", "answer": 0}]}, {"id": 252, "question": "Which Cloud Dataflow / Beam feature should you use to aggregate data in an unbounded data source every hour based on the time when the data entered the pipeline?", "choices": [{"id": 2554, "choice": " An hourly watermark\n", "answer": 0}, {"id": 2555, "choice": "An event time trigger\n", "answer": 0}, {"id": 2556, "choice": "The with Allowed Lateness method\n", "answer": 0}, {"id": 2557, "choice": "A processing time trigger", "answer": 1}]}, {"id": 253, "question": "Which of the following is NOT true about Dataflow pipelines?", "choices": [{"id": 2558, "choice": " Dataflow pipelines are tied to Dataflow, and cannot be run on any other runner\n", "answer": 1}, {"id": 2559, "choice": "Dataflow pipelines can consume data from other Google Cloud services\n", "answer": 0}, {"id": 2560, "choice": "Dataflow pipelines can be programmed in Java\n", "answer": 0}, {"id": 2561, "choice": "Dataflow pipelines use a unified programming model, so can work both with streaming and batch data sources", "answer": 0}]}, {"id": 254, "question": "You are developing a software application using Google's Dataflow SDK, and want to use conditional, for loops and other complex programming structures to create a branching pipeline. Which component will be used for the data processing operation?", "choices": [{"id": 2562, "choice": " PCollection ", "answer": 0}, {"id": 2563, "choice": "Transform ", "answer": 1}, {"id": 2564, "choice": "Pipeline\n", "answer": 0}, {"id": 2565, "choice": "Sink API", "answer": 0}]}, {"id": 255, "question": "Which of the following IAM roles does your Compute Engine account require to be able to run pipeline jobs?", "choices": [{"id": 2566, "choice": " dataflow.worker\n", "answer": 1}, {"id": 2567, "choice": "dataflow.compute ", "answer": 0}, {"id": 2568, "choice": "dataflow.developer ", "answer": 0}, {"id": 2569, "choice": "dataflow.viewer", "answer": 0}]}, {"id": 256, "question": "Which of the following is not true about Dataflow pipelines?", "choices": [{"id": 2570, "choice": " Pipelines are a set of operations\n", "answer": 0}, {"id": 2571, "choice": "Pipelines represent a data processing job\n", "answer": 0}, {"id": 2572, "choice": "Pipelines represent a directed graph of steps\n", "answer": 0}, {"id": 2573, "choice": "Pipelines can share data between instances", "answer": 1}]}, {"id": 257, "question": "By default, which of the following windowing behavior does Dataflow apply to unbounded data sets?", "choices": [{"id": 2574, "choice": " Windows at every 100 MB of data\n", "answer": 0}, {"id": 2575, "choice": "Single, Global W indow\n", "answer": 1}, {"id": 2576, "choice": "Windows at every 1 minute\n", "answer": 0}, {"id": 2577, "choice": "Windows at every 10 minutes", "answer": 0}]}, {"id": 258, "question": "Which of the following job types are supported by Cloud Dataproc (select 3 answers)?", "choices": [{"id": 2578, "choice": " Hive ", "answer": 1}, {"id": 2579, "choice": "Pig\n", "answer": 1}, {"id": 2580, "choice": "YARN ", "answer": 0}, {"id": 2581, "choice": "Spark", "answer": 1}]}, {"id": 259, "question": "What are the minimum permissions needed for a service account used with Google Dataproc?", "choices": [{"id": 2582, "choice": " Execute to Google Cloud Storage; write to Google Cloud Logging\n", "answer": 0}, {"id": 2583, "choice": "Write to Google Cloud Storage; read to Google Cloud Logging\n", "answer": 0}, {"id": 2584, "choice": "Execute to Google Cloud Storage; execute to Google Cloud Logging\n", "answer": 0}, {"id": 2585, "choice": "Read and write to Google Cloud Storage; write to Google Cloud Logging", "answer": 1}]}, {"id": 260, "question": "Which role must be assigned to a service account used by the virtual machines in a Dataproc cluster so they can execute jobs?", "choices": [{"id": 2586, "choice": " Dataproc Worker\n", "answer": 1}, {"id": 2587, "choice": "Dataproc Viewer\n", "answer": 0}, {"id": 2588, "choice": "Dataproc Runner\n", "answer": 0}, {"id": 2589, "choice": "Dataproc Editor", "answer": 0}]}, {"id": 261, "question": "When creating a new Cloud Dataproc cluster with the projects.regions.clusters.create operation, these four values are required: project, region, name, and ____.", "choices": [{"id": 2590, "choice": " zone ", "answer": 1}, {"id": 2591, "choice": "node ", "answer": 0}, {"id": 2592, "choice": "label ", "answer": 0}, {"id": 2593, "choice": "type", "answer": 0}]}, {"id": 262, "question": "Which Google Cloud Platform service is an alternative to Hadoop with Hive?", "choices": [{"id": 2594, "choice": " Cloud Dataflow\n", "answer": 0}, {"id": 2595, "choice": "Cloud Bigtable\n", "answer": 0}, {"id": 2596, "choice": "BigQuery\n", "answer": 1}, {"id": 2597, "choice": "Cloud Datastore", "answer": 0}]}, {"id": 263, "question": "Which of these rules apply when you add preemptible workers to a Dataproc cluster (select 2 answers)?", "choices": [{"id": 2598, "choice": " Preemptible workers cannot use persistent disk.\n", "answer": 0}, {"id": 2599, "choice": "Preemptible workers cannot store data.\n", "answer": 1}, {"id": 2600, "choice": "If a preemptible worker is reclaimed, then a replacement worker must be added manually.\n", "answer": 0}, {"id": 2601, "choice": "A Dataproc cluster cannot have only preemptible workers.", "answer": 1}]}, {"id": 264, "question": "When using Cloud Dataproc clusters, you can access the YARN web interface by configuring a browser to connect through a ____ proxy.", "choices": [{"id": 2602, "choice": " HTTPS ", "answer": 0}, {"id": 2603, "choice": "VPN\n", "answer": 0}, {"id": 2604, "choice": "SOCKS ", "answer": 1}, {"id": 2605, "choice": "HTTP", "answer": 0}]}, {"id": 265, "question": "Cloud Dataproc is a managed Apache Hadoop and Apache _____ service.", "choices": [{"id": 2606, "choice": " Blaze ", "answer": 0}, {"id": 2607, "choice": "Spark ", "answer": 1}, {"id": 2608, "choice": "Fire ", "answer": 0}, {"id": 2609, "choice": "Ignite", "answer": 0}]}, {"id": 266, "question": "Which action can a Cloud Dataproc Viewer perform?", "choices": [{"id": 2610, "choice": " Submit a job.\n", "answer": 0}, {"id": 2611, "choice": "Create a cluster.\n", "answer": 0}, {"id": 2612, "choice": "Delete a cluster.\n", "answer": 0}, {"id": 2613, "choice": "List the jobs.", "answer": 1}]}, {"id": 267, "question": "Dataproc clusters contain many configuration files. To update these files, you will need to use the --properties option. The format for the option is: file_prefix:property=_____.", "choices": [{"id": 2614, "choice": " details ", "answer": 0}, {"id": 2615, "choice": "value ", "answer": 1}, {"id": 2616, "choice": "null\n", "answer": 0}, {"id": 2617, "choice": "id", "answer": 0}]}, {"id": 268, "question": "Scaling a Cloud Dataproc cluster typically involves ____.", "choices": [{"id": 2618, "choice": " increasing or decreasing the number of worker nodes\n", "answer": 1}, {"id": 2619, "choice": "increasing or decreasing the number of master nodes\n", "answer": 0}, {"id": 2620, "choice": "moving memory to run more applications on a single node\n", "answer": 0}, {"id": 2621, "choice": "deleting applications from unused nodes periodically", "answer": 0}]}, {"id": 269, "question": "Cloud Dataproc charges you only for what you really use with _____ billing.", "choices": [{"id": 2622, "choice": " month-by-month ", "answer": 0}, {"id": 2623, "choice": "minute-by-minute ", "answer": 1}, {"id": 2624, "choice": "week-by-week\n", "answer": 0}, {"id": 2625, "choice": "hour-by-hour", "answer": 0}]}, {"id": 270, "question": "The YARN ResourceManager and the HDFS NameNode interfaces are available on a Cloud Dataproc cluster ____.", "choices": [{"id": 2626, "choice": " application node\n", "answer": 0}, {"id": 2627, "choice": "conditional node\n", "answer": 0}, {"id": 2628, "choice": "master node\n", "answer": 1}, {"id": 2629, "choice": "worker node", "answer": 0}]}, {"id": 271, "question": "Which of these is NOT a way to customize the software on Dataproc cluster instances?", "choices": [{"id": 2630, "choice": " Set initialization actions\n", "answer": 0}, {"id": 2631, "choice": "Modify configuration files using cluster properties\n", "answer": 0}, {"id": 2632, "choice": "Configure the cluster using Cloud Deployment Manager\n", "answer": 1}, {"id": 2633, "choice": "Log into the master node and make changes from there", "answer": 0}]}, {"id": 272, "question": "In order to securely transfer web traffic data from your computer's web browser to the Cloud Dataproc cluster you should use a(n) _____.", "choices": [{"id": 2634, "choice": " VPN connection\n", "answer": 0}, {"id": 2635, "choice": "Special browser\n", "answer": 0}, {"id": 2636, "choice": "SSH tunnel\n", "answer": 1}, {"id": 2637, "choice": "FTP connection", "answer": 0}]}, {"id": 273, "question": "All Google Cloud Bigtable client requests go through a front-end server ______ they are sent to a Cloud Bigtable node.", "choices": [{"id": 2638, "choice": " before ", "answer": 1}, {"id": 2639, "choice": "after ", "answer": 0}, {"id": 2640, "choice": "onlyif ", "answer": 0}, {"id": 2641, "choice": "once", "answer": 0}]}, {"id": 274, "question": "What is the general recommendation when designing your row keys for a Cloud Bigtable schema?", "choices": [{"id": 2642, "choice": " Include multiple time series values within the row key\n", "answer": 0}, {"id": 2643, "choice": "Keep the row keep as an 8 bit integer\n", "answer": 0}, {"id": 2644, "choice": "Keepyourrowkeyreasonablyshort\n", "answer": 1}, {"id": 2645, "choice": "Keep your row key as long as the field permits", "answer": 0}]}, {"id": 275, "question": "Which of the following statements is NOT true regarding Bigtable access roles?", "choices": [{"id": 2646, "choice": " Using IAM roles, you cannot give a user access to only one table in a project, rather than all tables in a project.\n", "answer": 0}, {"id": 2647, "choice": "To give a user access to only one table in a project, grant the user the Bigtable Editor role for that table.\n", "answer": 1}, {"id": 2648, "choice": "You can configure access control only at the project level.\n", "answer": 0}, {"id": 2649, "choice": "To give a user access to only one table in a project, you must configure access through your application.", "answer": 0}]}, {"id": 276, "question": "For the best possible performance, what is the recommended zone for your Compute Engine instance and Cloud Bigtable instance?", "choices": [{"id": 2650, "choice": " Have the Compute Engine instance in the furthest zone from the Cloud Bigtable instance.\n", "answer": 0}, {"id": 2651, "choice": "Have both the Compute Engine instance and the Cloud Bigtable instance to be in different zones.\n", "answer": 0}, {"id": 2652, "choice": "Have both the Compute Engine instance and the Cloud Bigtable instance to be in the same zone.\n", "answer": 1}, {"id": 2653, "choice": "Have the Cloud Bigtable instance to be in the same zone as all of the consumers of your data.", "answer": 0}]}, {"id": 277, "question": "Which row keys are likely to cause a disproportionate number of reads and/or writes on a particular node in a Bigtable cluster (select 2 answers)?", "choices": [{"id": 2654, "choice": " A sequential numeric ID\n", "answer": 1}, {"id": 2655, "choice": "A timestamp followed by a stock symbol\n", "answer": 1}, {"id": 2656, "choice": "A non-sequential numeric ID\n", "answer": 0}, {"id": 2657, "choice": "A stock symbol followed by a timestamp", "answer": 0}]}, {"id": 278, "question": "When a Cloud Bigtable node fails, ____ is lost.", "choices": [{"id": 2658, "choice": " all data\n", "answer": 0}, {"id": 2659, "choice": "no data\n", "answer": 1}, {"id": 2660, "choice": "the last transaction\n", "answer": 0}, {"id": 2661, "choice": "the time dimension", "answer": 0}]}, {"id": 279, "question": "Which is not a valid reason for poor Cloud Bigtable performance?", "choices": [{"id": 2662, "choice": " The workload isn't appropriate for Cloud Bigtable.\n", "answer": 0}, {"id": 2663, "choice": "The table's schema is not designed correctly.\n", "answer": 0}, {"id": 2664, "choice": "The Cloud Bigtable cluster has too many nodes.\n", "answer": 1}, {"id": 2665, "choice": "There are issues with the network connection.", "answer": 0}]}, {"id": 280, "question": "Which is the preferred method to use to avoid hotspotting in time series data in Bigtable?", "choices": [{"id": 2666, "choice": " Field promotion\n", "answer": 1}, {"id": 2667, "choice": "Randomization\n", "answer": 0}, {"id": 2668, "choice": "Salting\n", "answer": 0}, {"id": 2669, "choice": "Hashing", "answer": 0}]}, {"id": 281, "question": "When you design a Google Cloud Bigtable schema it is recommended that you _________.", "choices": [{"id": 2670, "choice": " Avoid schema designs that are based on NoSQL concepts\n", "answer": 0}, {"id": 2671, "choice": "Create schema designs that are based on a relational database design\n", "answer": 0}, {"id": 2672, "choice": "Avoid schema designs that require atomicity across rows\n", "answer": 1}, {"id": 2673, "choice": "Create schema designs that require atomicity across rows", "answer": 0}]}, {"id": 282, "question": "Which of the following is NOT a valid use case to select HDD (hard disk drives) as the storage for Google Cloud Bigtable?", "choices": [{"id": 2674, "choice": " You expect to store at least 10 TB of data.\n", "answer": 0}, {"id": 2675, "choice": "You will mostly run batch workloads with scans and writes, rather than frequently executing random reads of a small number of rows.\n", "answer": 0}, {"id": 2676, "choice": "You need to integrate with Google BigQuery.\n", "answer": 1}, {"id": 2677, "choice": "You will not use the data to back a user-facing or latency-sensitive application.", "answer": 0}]}, {"id": 283, "question": "Cloud Bigtable is Google's ______ Big Data database service.", "choices": [{"id": 2678, "choice": " Relational ", "answer": 0}, {"id": 2679, "choice": "mySQL\n", "answer": 0}, {"id": 2680, "choice": "NoSQL\n", "answer": 1}, {"id": 2681, "choice": "SQL Server", "answer": 0}]}, {"id": 284, "question": "When you store data in Cloud Bigtable, what is the recommended minimum amount of stored data?", "choices": [{"id": 2682, "choice": " 500 TB\n", "answer": 0}, {"id": 2683, "choice": "1GB\n", "answer": 0}, {"id": 2684, "choice": "1TB\n", "answer": 1}, {"id": 2685, "choice": "500 GB", "answer": 0}]}, {"id": 285, "question": "If you're running a performance test that depends upon Cloud Bigtable, all the choices except one below are recommended steps. Which is NOT a recommended step to follow?", "choices": [{"id": 2686, "choice": " Do not use a production instance.\n", "answer": 1}, {"id": 2687, "choice": "Run your test for at least 10 minutes.\n", "answer": 0}, {"id": 2688, "choice": "Before you test, run a heavy pre-test for several minutes.\n", "answer": 0}, {"id": 2689, "choice": "Use at least 300 GB of data.", "answer": 0}]}, {"id": 286, "question": "Cloud Bigtable is a recommended option for storing very large amounts of ____________________________?", "choices": [{"id": 2690, "choice": " multi-keyed data with very high latency\n", "answer": 0}, {"id": 2691, "choice": "multi-keyed data with very low latency\n", "answer": 0}, {"id": 2692, "choice": "single-keyed data with very low latency\n", "answer": 1}, {"id": 2693, "choice": "single-keyed data with very high latency", "answer": 0}]}, {"id": 287, "question": "Google Cloud Bigtable indexes a single value in each row. This value is called the _______.", "choices": [{"id": 2694, "choice": " primary key\n", "answer": 0}, {"id": 2695, "choice": "unique key\n", "answer": 0}, {"id": 2696, "choice": "row key\n", "answer": 1}, {"id": 2697, "choice": "master key", "answer": 0}]}, {"id": 288, "question": "What is the HBase Shell for Cloud Bigtable?", "choices": [{"id": 2698, "choice": " The HBase shell is a GUI based interface that performs administrative tasks, such as creating and deleting tables.\n", "answer": 0}, {"id": 2699, "choice": "The HBase shell is a command-line tool that performs administrative tasks, such as creating and deleting tables.\n", "answer": 1}, {"id": 2700, "choice": "The HBase shell is a hypervisor based shell that performs administrative tasks, such as creating and deleting new virtualized instances.\n", "answer": 0}, {"id": 2701, "choice": "The HBase shell is a command-line tool that performs only user account management functions to grant access to Cloud Bigtable instances.", "answer": 0}]}, {"id": 289, "question": "What is the recommended action to do in order to switch between SSD and HDD storage for your Google Cloud Bigtable instance?", "choices": [{"id": 2702, "choice": " create a third instance and sync the data from the two storage types via batch jobs\n", "answer": 0}, {"id": 2703, "choice": "export the data from the existing instance and import the data into a new instance\n", "answer": 1}, {"id": 2704, "choice": "run parallel instances where one is HDD and the other is SDD\n", "answer": 0}, {"id": 2705, "choice": "the selection is final and you must resume using the same storage type", "answer": 0}]}]}