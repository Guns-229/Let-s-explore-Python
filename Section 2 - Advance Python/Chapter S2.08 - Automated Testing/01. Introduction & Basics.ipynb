{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing your code is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting used to writing testing code and running this code in parallel is now considered a good habit. Used wisely, this method helps you define more precisely your code’s intent and have a more decoupled architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Rules of Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gist from From: https://github.com/kennethreitz/python-guide/blob/master/docs/writing/tests.rst\n",
    "- Each test case must test only one small independent functionality of the code.\n",
    "- Each test must be able to run alone, and also along with the test suite irrespective of the order in which they are called.\n",
    "- Clear your own mess. Testcases should clean the app after the execution is over.\n",
    "- Tests must run as fast as possible without compromising. In cases, where its not possible to achieve it club/mark them so they can be executed only when required, or can be run in parallel. \n",
    "- Learn your tools and learn how to run a single test or a test case. Then, when developing a function inside a module, run this function’s tests frequently, ideally automatically when you save the code.\n",
    "- Always run the full test suite before a coding session, and run it again after. This will give you more confidence that you did not break anything in the rest of the code.\n",
    "- It is a good idea to implement a hook that runs all tests before pushing code to a shared repository.\n",
    "- If you are in the middle of a development session and have to interrupt your work, it is a good idea to write a broken unit test about what you want to develop next. When coming back to work, you will have a pointer to where you were and get back on track faster.\n",
    "- The first step when you are debugging your code is to write a new test pinpointing the bug. While it is not always possible to do, those bug catching tests are among the most valuable pieces of code in your project.\n",
    "- Use long and descriptive names for testing functions. The style guide here is slightly different than that of running code, where short names are often preferred. The reason is testing functions are never called explicitly. square() or even sqr() is ok in running code, but in testing code you would have names such as test_square_of_number_2(), test_square_negative_number(). These function names are displayed when a test fails, and should be as descriptive as possible.\n",
    "- When something goes wrong or has to be changed, and if your code has a good set of tests, you or other maintainers will rely largely on the testing suite to fix the problem or modify a given behavior. Therefore the testing code will be read as much as or even more than the running code. A unit test whose purpose is unclear is not very helpful in this case.\n",
    "- Another use of the testing code is as an introduction to new developers. When someone will have to work on the code base, running and reading the related testing code is often the best thing that they can do to start. They will or should discover the hot spots, where most difficulties arise, and the corner cases. If they have to add some functionality, the first step should be to add a test to ensure that the new functionality is not already a working path that has not been plugged into the interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unittest is the batteries-included test module in the Python standard library. Its API will be familiar to anyone who has used any of the JUnit/nUnit/CppUnit series of tools.\n",
    "\n",
    "Creating test cases is accomplished by subclassing unittest.TestCase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import TestCase\n",
    "\n",
    "def fun(x):\n",
    "    return x + 1\n",
    "\n",
    "class MyTest(TestCase):\n",
    "    def setUp(self):\n",
    "        pass\n",
    "    \n",
    "    def tearDown(self):\n",
    "        pass\n",
    "    \n",
    "    def test_passing_int_value(self):\n",
    "        self.assertEqual(fun(3), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doctest\n",
    "The doctest module searches for pieces of text that look like interactive Python sessions in docstrings, and then executes those sessions to verify that they work exactly as shown.\n",
    "\n",
    "Doctests have a different use case than proper unit tests: they are usually less detailed and don’t catch special cases or obscure regression bugs. They are useful as an expressive documentation of the main use cases of a module and its components. However, doctests should run automatically each time the full test suite runs.\n",
    "\n",
    "A simple doctest in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 12, in __main__.square\n",
      "Failed example:\n",
      "    square(-1)\n",
      "Expected:\n",
      "    2\n",
      "Got:\n",
      "    1\n",
      "**********************************************************************\n",
      "1 items had failures:\n",
      "   1 of   3 in __main__.square\n",
      "***Test Failed*** 1 failures.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=1, attempted=3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    \"\"\"Return the square of x.\n",
    "    \n",
    "    this should return 4 as 2*2 = 4\n",
    "    >>> square(2)\n",
    "    4\n",
    "    \n",
    "    >>> square(-2)\n",
    "    4\n",
    "    \n",
    "    This will return 1\n",
    "    >>> square(-1)\n",
    "    2\n",
    "    \"\"\"\n",
    "\n",
    "    return x * x\n",
    "\n",
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 5, in __main__.listme\n",
      "Failed example:\n",
      "    listme(2)\n",
      "Expected:\n",
      "    4\n",
      "Got:\n",
      "    (2, 2)\n",
      "**********************************************************************\n",
      "File \"__main__\", line 8, in __main__.listme\n",
      "Failed example:\n",
      "    listme(-2)\n",
      "Expected:\n",
      "    4\n",
      "Got:\n",
      "    (-2, -2)\n",
      "**********************************************************************\n",
      "File \"__main__\", line 12, in __main__.listme\n",
      "Failed example:\n",
      "    listme(-1)\n",
      "Expected:\n",
      "    [-1, -1]\n",
      "Got:\n",
      "    (-1, -1)\n",
      "**********************************************************************\n",
      "File \"__main__\", line 12, in __main__.square\n",
      "Failed example:\n",
      "    square(-1)\n",
      "Expected:\n",
      "    2\n",
      "Got:\n",
      "    1\n",
      "**********************************************************************\n",
      "2 items had failures:\n",
      "   3 of   3 in __main__.listme\n",
      "   1 of   3 in __main__.square\n",
      "***Test Failed*** 4 failures.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=4, attempted=6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def listme(x):\n",
    "    \"\"\"Return the square of x.\n",
    "    \n",
    "    this should return 4 as 2*2 = 4\n",
    "    >>> listme(2)\n",
    "    4\n",
    "    \n",
    "    >>> listme(-2)\n",
    "    4\n",
    "    \n",
    "    This will return 1\n",
    "    >>> listme(-1)\n",
    "    [-1, -1]\n",
    "    \"\"\"\n",
    "    \n",
    "    return x, x\n",
    "\n",
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this module from the command line as in python module.py, the doctests will run and complain if anything is not behaving as described in the docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Room(object):\n",
    "\n",
    "    def __init__(self, name, description):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.paths = {}\n",
    "\n",
    "    def go(self, direction):\n",
    "        return self.paths.get(direction, None)\n",
    "\n",
    "    def add_paths(self, paths):\n",
    "        self.paths.update(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Testing Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from above, in-built provided tools, python community has developed few more testing frameworks we are going to give a brief of few of them in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### `py.test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seperate module for it thus not covering it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `nose`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nose.tools import *\n",
    "\n",
    "\n",
    "def test_room():\n",
    "    gold = Room(\"GoldRoom\",\n",
    "                \"\"\"This room has gold in it you can grab. There's a\n",
    "                door to the north.\"\"\")\n",
    "    assert_equal(gold.name, \"GoldRoom\")\n",
    "    assert_equal(gold.paths, {})\n",
    "\n",
    "def test_room_paths():\n",
    "    center = Room(\"Center\", \"Test room in the center.\")\n",
    "    north = Room(\"North\", \"Test room in the north.\")\n",
    "    south = Room(\"South\", \"Test room in the south.\")\n",
    "\n",
    "    center.add_paths({'north': north, 'south': south})\n",
    "    assert_equal(center.go('north'), north)\n",
    "    assert_equal(center.go('south'), south)\n",
    "\n",
    "def test_map():\n",
    "    start = Room(\"Start\", \"You can go west and down a hole.\")\n",
    "    west = Room(\"Trees\", \"There are trees here, you can go east.\")\n",
    "    down = Room(\"Dungeon\", \"It's dark down here, you can go up.\")\n",
    "\n",
    "    start.add_paths({'west': west, 'down': down})\n",
    "    west.add_paths({'east': start})\n",
    "    down.add_paths({'up': start})\n",
    "\n",
    "    assert_equal(start.go('west'), west)\n",
    "    assert_equal(start.go('west').go('east'), start)\n",
    "    assert_equal(start.go('down').go('up'), start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import logging\n",
    "import unittest\n",
    "\n",
    "log = logging.getLogger()\n",
    "\n",
    "from nose import core, loader\n",
    "\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "from types import ModuleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nose import SkipTest\n",
    "\n",
    "def test_foo():\n",
    "    assert True\n",
    "    \n",
    "def test_bar():\n",
    "    assert False\n",
    "    \n",
    "def test_baz():\n",
    "    raise SkipTest(\"Skipped\")\n",
    "    \n",
    "import random\n",
    "import time\n",
    "def test_generate():\n",
    "    for _ in range(random.randint(0, 10)):\n",
    "        time.sleep(0.25)\n",
    "        yield lambda x: None, _\n",
    "    def fail(x):\n",
    "        time.sleep(0.25)\n",
    "        raise AssertionError(\"Failed\")\n",
    "    for _ in range(random.randint(0, 10)):\n",
    "        yield fail, _\n",
    "    def skip(x):\n",
    "        time.sleep(0.25)\n",
    "        raise SkipTest(\"Skipped\")\n",
    "    for _ in range(random.randint(0, 10)):\n",
    "        yield skip, _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asserts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's `assert` checks a condition, if it is `false` it raises an `AssertionError` with an optional error message as shown in the below example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Without any error message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = 10\n",
    "try:\n",
    "    assert x > 10\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **With custom error message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is greater than 10\n"
     ]
    }
   ],
   "source": [
    "x = 10\n",
    "try:\n",
    "    assert x > 10, \"x is greater than 10\"\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0b5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
